{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeet1912/ms/blob/main/ds677/assignments/week2/DS677_Week2HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcY5TDrIZI1h"
      },
      "source": [
        "# **Week 2 Homework 2: S&P 500 Index Prediction**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objectives:** Predict the next value of the S&P 500 index based on historical data points.\n",
        "\n",
        "The S&P 500, also known as the Standard & Poor's 500, is a stock market index representing the performance of 500 major companies listed on U.S. exchanges. We aim to train the linear regression model on the training dataset using gradient descent. We will use the test set as a validation set to determine the optimal number of features, `Sequence Length`, and the corresponding model parameters.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fn84qc3IBYOp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "----\n",
        "\n",
        "`Sequence Length` refers to the number of historical data points used to predict the next value in a time series. A further explanationï¼šGiven the previous N data points, you are going to predict the next value. N can be 1, ..., n. Here, N is the sequence length\n",
        "\n",
        "\n",
        "\n",
        "**Your task of this homework will be:**\n",
        "1.   Independently implement gradient descent method\n",
        "2.   Choose the optimal value for `Sequence Length` and familiarize yourself with model tuning"
      ],
      "metadata": {
        "id": "n_VL8n1Uvdvf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Data\n",
        "Download data from google drive\n",
        "\n",
        "You should have\n",
        "- `SPY_dataset.csv`: the dataset, which includes the date and corresponding SPY close prices.\n",
        "\n",
        "after running the following block."
      ],
      "metadata": {
        "id": "cTSo9IC6BrtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade gdown\n",
        "\n",
        "# Main link\n",
        "!gdown --id '1UH1H8dmYuOcfPRPVDYLI1AIIcwPoqVqW' --output SPY_dataset.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3d8FzSeSvGg",
        "outputId": "3039008e-84b3-4309-ab10-6ef37e94ebbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.6)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.8.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1UH1H8dmYuOcfPRPVDYLI1AIIcwPoqVqW\n",
            "To: /content/SPY_dataset.csv\n",
            "100% 25.4k/25.4k [00:00<00:00, 72.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "BIu5oijQUKwP",
        "outputId": "60b82f78-32fc-4939-9023-4fcf0552011d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  SPY_dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some Utilities\n",
        "\n",
        "Plotly is a graphing library for Python; we will use it later to display the final result."
      ],
      "metadata": {
        "id": "ZfsHYQx2U7rl"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMx5JzRBP20Y",
        "outputId": "dce10190-ebeb-4a31-93d8-d45e2f39c3c7"
      },
      "source": [
        "!pip install plotly --upgrade"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (5.24.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly) (9.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly) (24.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the necessary libraries."
      ],
      "metadata": {
        "id": "lMOQLrqgVudp"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l81q2bvJYHlA"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import plotly.graph_objects as go\n",
        "import tensorflow as tf\n",
        "\n",
        "np.random.seed(777)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_tpu():\n",
        "    try:\n",
        "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "        print('Running on TPU ', tpu.master())\n",
        "    except ValueError:\n",
        "        tpu = None\n",
        "\n",
        "    if tpu:\n",
        "        tf.config.experimental_connect_to_cluster(tpu)\n",
        "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "        strategy = tf.distribute.TPUStrategy(tpu)\n",
        "    else:\n",
        "        print(\"TPU not found, using CPU/GPU\")\n",
        "        strategy = tf.distribute.get_strategy()\n",
        "\n",
        "    return strategy\n",
        "\n",
        "strategy = setup_tpu()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VYiDpUK7PMV",
        "outputId": "86e7f6a4-c06c-4a82-c065-4bc667994d77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:TPU system local has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on TPU  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFsq2YIQ2vyX"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "We load the dataset and divide it into a training set and a test set.\n",
        "\n",
        "\n",
        "*   The training set covers the period from 2017 to 2020\n",
        "*   The testing set covers the period from January 2021 to August 2021\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJZDcNZXYLIo",
        "outputId": "f23d8fac-3ee6-4b75-c43a-4710db7d08c7"
      },
      "source": [
        "# Load dataframe\n",
        "df = pd.read_csv('SPY_dataset.csv',\n",
        "                 parse_dates=['Date'])\n",
        "\n",
        "mask = (df['Date'] >= '2017-01-01') & (df['Date'] <= '2020-12-31')\n",
        "df_train = df.loc[mask]\n",
        "df_test = df.loc[~mask]\n",
        "\n",
        "print(\"The columns of training dataset:\", df_train.columns, \"The size of training dataset:\", df_train.shape)\n",
        "print(\"The columns of training dataset:\", df_test.columns, \"The size of testing dataset:\", df_test.shape)\n",
        "\n",
        "\n",
        "# Convert the price data into a list for later analysis\n",
        "data_train = df_train['Close'].to_list()\n",
        "data_test = df_test['Close'].to_list()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The columns of training dataset: Index(['Date', 'Close'], dtype='object') The size of training dataset: (1007, 2)\n",
            "The columns of training dataset: Index(['Date', 'Close'], dtype='object') The size of testing dataset: (166, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model\n",
        "\n",
        "Linear Regression by Sklearn library\n",
        "\n",
        "In this section, We will use existing models from the [sklearn](https://https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) library to make predictions.\n",
        "\n",
        "-----\n",
        "\n",
        "The parameter:\n",
        "\n",
        "```\n",
        "max_sequence_length=150\n",
        "```\n",
        " is used to explore the impact of different historical lengths, testing up to 150 days to assess various cyclical influences in stock market analysis. You are free to make changes. Remember, our goal is to find the best `sequence_length`. Once we have evaluated all possible candidates, we will be able to choose the best value for `sequence_length`.\n",
        "\n",
        " You can freely adjust the `max_sequence_length`, but keep in mind that longer lengths will increase the time it takes for the code to run.\n",
        "\n"
      ],
      "metadata": {
        "id": "Fpm18FPwXmuG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3hKLpLA2mXZ"
      },
      "source": [
        "def preprocess_data(price_data, sequence_length=6):\n",
        "    \"\"\" Prepare data for model training and evaluation by creating sequences of given length.\n",
        "\n",
        "    Args:\n",
        "        price_data (list): List of stock prices.\n",
        "        sequence_length (int): Number of historical points used to predict the next point.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Features and target arrays prepared for regression.\n",
        "    \"\"\"\n",
        "    num_data_points = len(price_data)\n",
        "    samples = []\n",
        "    for i in range(num_data_points - sequence_length + 1):\n",
        "        sample = price_data[i: i + sequence_length]\n",
        "        samples.append(sample)\n",
        "    samples = np.array(samples)\n",
        "    features = samples[:, :-1]\n",
        "    target = samples[:, -1]\n",
        "    return features, target\n",
        "\n",
        "def train_and_evaluate(data_train, data_test, max_sequence_length=150):\n",
        "    \"\"\" Train and evaluate linear regression model using training and testing datasets.\n",
        "\n",
        "    Args:\n",
        "        data_train (list): Training dataset prices.\n",
        "        data_test (list): Testing dataset prices.\n",
        "        max_sequence_length (int): Maximum length of data sequences used for training.\n",
        "\n",
        "    Returns:\n",
        "        dict: Contains various metrics and model parameters for each sequence length.\n",
        "    \"\"\"\n",
        "\n",
        "    results = {\n",
        "        'sequence_length': [],\n",
        "        'train_loss': [],\n",
        "        'test_loss': [],\n",
        "        'model_intercepts': [],\n",
        "        'model_coefficients': []\n",
        "    }\n",
        "\n",
        "    # Evaluate model performance across different sequence lengths\n",
        "    for length in range(1, max_sequence_length + 1):\n",
        "        features_train, target_train = preprocess_data(data_train, length + 1)\n",
        "        model = LinearRegression().fit(features_train, target_train)\n",
        "        predictions_train = model.predict(features_train)\n",
        "        train_loss = mean_squared_error(target_train, predictions_train)\n",
        "\n",
        "        features_test, target_test = preprocess_data(data_test, length + 1)\n",
        "        predictions_test = model.predict(features_test)\n",
        "        test_loss = mean_squared_error(target_test, predictions_test)\n",
        "\n",
        "        # Collect results\n",
        "        results['sequence_length'].append(length)\n",
        "        results['train_loss'].append(train_loss)\n",
        "        results['test_loss'].append(test_loss)\n",
        "        results['model_intercepts'].append(model.intercept_)\n",
        "        results['model_coefficients'].append(model.coef_)\n",
        "\n",
        "    return results\n",
        "\n",
        "# Example of function usage\n",
        "results = train_and_evaluate(data_train, data_test, max_sequence_length=150)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In this section, we will visually assess our prediction results through plotting."
      ],
      "metadata": {
        "id": "Sc297Ke3gSwP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FR_lu6H3YqlI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "420a2416-50ff-4129-d025-6541b1bd0f86"
      },
      "source": [
        "# Initialize a Plotly figure\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add training data trace\n",
        "fig.add_trace(\n",
        "    go.Scatter(\n",
        "        x=results['sequence_length'],\n",
        "        y=results['train_loss'],\n",
        "        mode='lines+markers',\n",
        "        line=dict(color='red'),\n",
        "        name='Training Loss'\n",
        "    )\n",
        ")\n",
        "\n",
        "# Add testing data trace\n",
        "fig.add_trace(\n",
        "    go.Scatter(\n",
        "        x=results['sequence_length'],\n",
        "        y=results['test_loss'],\n",
        "        mode='lines+markers',\n",
        "        line=dict(color='blue'),\n",
        "        name='Testing Loss'\n",
        "    )\n",
        ")\n",
        "\n",
        "# Update plot layout for better readability and aesthetics\n",
        "fig.update_layout(\n",
        "    title=\"Performance of Linear Regression Model\",\n",
        "    xaxis_title=\"Number of Historical Points (N)\",\n",
        "    yaxis_title=\"Mean Squared Error (MSE)\",\n",
        "    legend_title=\"Dataset Type\",\n",
        "    font=dict(\n",
        "        family=\"Arial, monospace\",\n",
        "        size=18,\n",
        "        color=\"RebeccaPurple\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Display the figure\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"953f22cf-affb-43ee-9525-a826459a0c15\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"953f22cf-affb-43ee-9525-a826459a0c15\")) {                    Plotly.newPlot(                        \"953f22cf-affb-43ee-9525-a826459a0c15\",                        [{\"line\":{\"color\":\"red\"},\"mode\":\"lines+markers\",\"name\":\"Training Loss\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150],\"y\":[12.490499525232535,12.003281124415906,11.803635493207398,11.758709466623369,11.67048728086166,11.668503308249864,11.500671761278387,11.059671062150603,10.951081867125275,10.721264853212434,10.728377394636887,10.738179742735054,10.747775048687984,10.74332294728676,10.747940661759692,10.756417700572632,10.767176988223934,10.761101847101434,10.763902903158412,10.769071081819689,10.763436111700093,10.77180146033594,10.760916499295183,10.731033239888792,10.726600706562365,10.734520975588298,10.678744002301148,10.59511274593895,10.604291797582812,10.595200217194717,10.600557843827799,10.610613978837772,10.61511642406007,10.624595047948967,10.59768296338836,10.597743456873607,10.582638339599503,10.593508705873559,10.603130993917036,10.595351569134838,10.589544554347373,10.557739824521985,10.558480541868478,10.561615804003852,10.510854817226944,10.520493183724971,10.465618985735745,10.474048029062216,10.477210655561354,10.483204785777101,10.489600986712079,10.495291501590291,10.48683398218445,10.486182116490037,10.461339345740248,10.467924807707387,10.478205918197292,10.482070247603376,10.467018862254958,10.477089600049409,10.482667691037006,10.410667495937544,10.412763619324114,10.422295114168872,10.420086480278112,10.423846895408996,10.43426804343248,10.444599467091043,10.455623044488176,10.461569007388347,10.46884586112528,10.455574731544349,10.466246298925663,10.465586354530004,10.47237576744017,10.416009971089435,10.423473068824038,10.386369166138833,10.397083561896926,10.356448499032085,10.367492529688377,10.356113866106261,10.343926774496254,10.350226027984277,10.355771807276108,10.350687804506641,10.360931258627877,10.37048165386089,10.381247707333667,10.391193041031999,10.402457705596587,10.40367782348471,10.415009902555202,10.405092304313845,10.376832422805652,10.371975508078155,10.377877558428896,10.381891930765828,10.382518330011601,10.375600318481084,10.37669247623261,10.387637797776936,10.341356817068784,10.345374373505262,10.354593826946438,10.365821920257444,10.368674746007969,10.366048412934205,10.369327307485054,10.379147553193897,10.387504080697727,10.397236344150546,10.40755215720018,10.40618255743336,10.383708097740124,10.372441610685923,10.361818005021469,10.373454480200829,10.36565452993155,10.373739404333806,10.384804474762152,10.386916040446678,10.39318536752795,10.40345729390351,10.402894091586335,10.410801687292878,10.38640297764915,10.387750590393296,10.392380725914805,10.38566595472691,10.366207738115722,10.376257133497738,10.346689328463851,10.332014075372074,10.343518174863856,10.326659627293646,10.323839080698411,10.334948308001353,10.34124149298912,10.35262242987049,10.363797037791297,10.35679181516957,10.368068479788379,10.37973449407272,10.377505516322929,10.383535297082593,10.395597187716003,10.37889822178998,10.388026535080476,10.399752097770875],\"type\":\"scatter\"},{\"line\":{\"color\":\"blue\"},\"mode\":\"lines+markers\",\"name\":\"Testing Loss\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150],\"y\":[10.719753718288816,10.680381906195246,11.293026328676053,11.466341451691301,11.644687475887054,11.686900003146693,11.502221008098887,12.229300242331782,12.111448558662676,12.242680091907083,12.291413838376199,12.163122069402048,12.23196989137229,12.15147984827413,12.21098526430477,12.277663271838254,11.866976760743766,12.118796892170348,11.958362625607526,11.871503524761541,11.79325959828907,11.899789078209675,11.822733552580159,11.895927311828803,12.038002778542003,12.078954533442726,12.016470379014015,12.347544360499354,12.419787738925141,12.586812233511697,12.621543099703983,12.64402105198224,12.719126058026399,12.789308499317238,13.081443640113026,13.018142508769591,12.53468504800986,12.558253233462544,12.1002706239426,12.139180323702979,11.941411166792598,11.673994252370969,11.268399097875365,11.325389282755058,11.154244422018797,11.127753353207812,11.258254626031004,11.361907796091163,11.284788864790487,11.293766907618748,11.372968597352603,10.953796503118157,11.158805541717873,11.23216756851426,11.305727137171797,11.176891735163336,11.197828657636823,10.938915124498896,11.05922326284766,10.856844527590876,10.953510531688156,10.946325233739383,10.886493726082817,10.94917666123191,11.144965064898571,11.153163981147681,10.964679339950093,11.087499324621389,11.001669732352658,11.097487335185477,11.121703184347293,11.203996348992918,11.307881547327764,11.23685781901719,11.179842985763587,11.54996239256689,10.965885460575285,10.882884305902234,10.991874132763762,11.363824914947532,11.415721080242154,11.562128769238356,11.709927140255896,11.61767782370933,11.695891683312862,11.77075146645705,11.459909334868184,11.329654942598859,11.395073340169912,10.173305053834094,9.396004009029344,8.819454814525596,8.93531506214349,7.854526551042702,8.381317553286141,8.393111889541183,8.461800316081106,8.424860217745717,8.804894234676166,8.980193612365268,8.972035585486534,9.138761665016583,9.883072642258233,10.044618847404717,10.093584022624938,9.916389418126986,10.107509987616355,10.115656261143352,10.341866316676672,10.168891294660924,10.373883541537234,10.634069731042345,10.590601358951876,10.856353358064577,11.053843624289634,9.856760285190807,9.894830909480559,9.429549507194855,9.382479948460222,9.724478377806872,9.933846365216795,10.051378772102353,10.274784726176785,10.52290011466042,10.343392182452993,10.423697433503214,10.43169905604088,10.727619781948684,10.81979619938555,10.661115853535286,11.061261523094032,11.366816172875492,11.574482347269724,12.099807534051706,11.810783753655434,10.14080961811269,8.237334970225039,7.825881575454851,8.163545199739152,8.05574798101566,8.030446063908862,8.06208220116533,8.285066638641352,8.632596437777375,9.317524536417759,9.447824762702842,9.93929748736884,10.151558157724331,10.83509997582648,11.140351280154078],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"font\":{\"family\":\"Arial, monospace\",\"size\":18,\"color\":\"RebeccaPurple\"},\"title\":{\"text\":\"Performance of Linear Regression Model\"},\"xaxis\":{\"title\":{\"text\":\"Number of Historical Points (N)\"}},\"yaxis\":{\"title\":{\"text\":\"Mean Squared Error (MSE)\"}},\"legend\":{\"title\":{\"text\":\"Dataset Type\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('953f22cf-affb-43ee-9525-a826459a0c15');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJGyOvEOdyRm"
      },
      "source": [
        "# Gradient Descent Implementation\n",
        "\n",
        "In this section, we will build a Linear Regression model from scratch.\n",
        "\n",
        "Complete the function for gradient descent according to the hints provided in the comments.\n",
        "\n",
        "Please write your own implementation wherever **'#Your Code'** appears."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zfcODVubXAu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "715b0ab2-7dca-4ede-9e5c-51f3ba82617c"
      },
      "source": [
        "def normalize_data(data): #to avoid overflow\n",
        "    return (data - np.mean(data)) / np.std(data)\n",
        "\n",
        "data_train_normalized = normalize_data(data_train)\n",
        "data_test_normalized = normalize_data(data_test)\n",
        "\n",
        "def predict(X, B):\n",
        "    \"\"\"\n",
        "    Predict outcomes using a linear model defined by coefficients B.\n",
        "\n",
        "    Args:\n",
        "        X (numpy.ndarray): Feature matrix for prediction.\n",
        "        B (numpy.ndarray): Coefficients of the model.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Predicted values.\n",
        "    \"\"\"\n",
        "    return np.array(X.dot(B), dtype=np.float64)\n",
        "\n",
        "\n",
        "def MSE(y_pred, y):\n",
        "    \"\"\"\n",
        "    Calculate the mean squared error between predicted and actual values.\n",
        "\n",
        "    Args:\n",
        "        y_pred (numpy.ndarray): Predicted values.\n",
        "        y (numpy.ndarray): Actual target values.\n",
        "\n",
        "    Returns:\n",
        "        float: Computed mean squared error.\n",
        "    \"\"\"\n",
        "    m = len(y_pred)\n",
        "    return np.sum((y_pred - y) ** 2) / m\n",
        "\n",
        "def gradient_descent(X, y, learning_rate=1e-7):\n",
        "    \"\"\"\n",
        "    Perform gradient descent to find the regression coefficients that minimize the MSE.\n",
        "\n",
        "    Args:\n",
        "        X (numpy.ndarray): Feature matrix.\n",
        "        y (numpy.ndarray): Target vector.\n",
        "        learning_rate (float): Step size for each iteration.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Optimal coefficients and the loss at convergence.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize coefficients as zeros with the same number of elements as there are features in X\n",
        "    B = np.zeros(X.shape[1])\n",
        "\n",
        "    # Set the initial previous mean squared error to infinity\n",
        "    mse_prev = float('inf')\n",
        "\n",
        "    # Start an infinite loop that will break once convergence criteria are met\n",
        "    while True:\n",
        "        # Calculate predicted values using the current coefficient values\n",
        "        # YOUR CODE\n",
        "        y_pred = predict(X,B)\n",
        "\n",
        "        # Compute the difference between predicted and actual values\n",
        "        # YOUR CODE\n",
        "        error = y_pred - y\n",
        "\n",
        "\n",
        "        # Calculate the gradient of the cost function\n",
        "        # YOUR CODE\n",
        "        grad = 2 / (X.shape[0]) * np.sum((np.transpose(X))*(error))\n",
        "        grad = np.array(grad, dtype=np.float64)\n",
        "\n",
        "        # Update the coefficients by taking a step proportional to the gradient\n",
        "        # YOUR CODE\n",
        "        B_next = B - learning_rate*grad\n",
        "        B_next = np.array(B_next, dtype=np.float64)\n",
        "\n",
        "        # Compute the mean squared error with the updated coefficients\n",
        "        # YOUR CODE\n",
        "        y_p = predict(X,B_next)\n",
        "        mse = MSE(y_p, y)\n",
        "\n",
        "        # Check if the change in MSE is below the threshold (learning rate), indicating convergence\n",
        "        # YOUR CODE\n",
        "        mse_diff = abs(mse_prev - mse)\n",
        "        if mse_diff < learning_rate:\n",
        "          break\n",
        "        # Update the previous MSE for the next iteration\n",
        "        # YOUR CODE\n",
        "        mse_prev = mse\n",
        "        B = B_next\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Return the optimized coefficients and the final mean squared error\n",
        "    return B, mse\n",
        "\n",
        "\n",
        "\n",
        "def train_and_evaluate(data_train, data_test, max_sequence_length=150, learning_rate=1e-7):\n",
        "    results = {\n",
        "        'sequence_length': [],\n",
        "        'train_loss': [],\n",
        "        'test_loss': [],\n",
        "        'model_intercepts': [],\n",
        "        'model_coefficients': []\n",
        "    }\n",
        "\n",
        "    for length in range(1, max_sequence_length + 1):\n",
        "        print(f'Processing Sequence Length={length}', end=\" \")\n",
        "\n",
        "        # Training\n",
        "        x_train, y_train = preprocess_data(data_train, length + 1)\n",
        "        x0_train = np.ones((x_train.shape[0], 1))\n",
        "        X_train = np.concatenate((x0_train, x_train), axis=1)\n",
        "\n",
        "        # Convert numpy arrays to TensorFlow tensors for TPU\n",
        "        X_train_tf = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "        y_train_tf = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
        "\n",
        "        with strategy.scope():  # Use TPU for computation\n",
        "            # Although gradient_descent uses numpy, we wrap it in TensorFlow's strategy\n",
        "            B, train_loss = gradient_descent(X_train_tf.numpy(), y_train_tf.numpy(), learning_rate)\n",
        "\n",
        "        # Convert back to numpy for storage in results\n",
        "        results['sequence_length'].append(length)\n",
        "        results['train_loss'].append(train_loss)\n",
        "        results['model_intercepts'].append(B[0])\n",
        "        results['model_coefficients'].append(B[1:])\n",
        "\n",
        "        # Testing\n",
        "        x_test, y_test = preprocess_data(data_test, length + 1)\n",
        "        x0_test = np.ones((x_test.shape[0], 1))\n",
        "        X_test = np.concatenate((x0_test, x_test), axis=1)\n",
        "\n",
        "        # Convert to TensorFlow tensors for prediction\n",
        "        X_test_tf = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
        "        y_test_tf = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
        "\n",
        "        y_pred_test = predict(X_test_tf.numpy(), B)\n",
        "        test_loss = MSE(y_pred_test, y_test_tf.numpy())\n",
        "\n",
        "        results['test_loss'].append(test_loss)\n",
        "\n",
        "        print(f\" Done. Train Loss: {train_loss:.6f}, Test Loss: {test_loss:.6f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# Lengths from 1 to 150\n",
        "learning_rate = 1e-5\n",
        "max_sequence_length = 150\n",
        "results = train_and_evaluate(data_train_normalized, data_test_normalized, max_sequence_length, learning_rate)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing Sequence Length=1  Done. Train Loss: 0.628802, Test Loss: 0.616278\n",
            "Processing Sequence Length=2  Done. Train Loss: 0.256551, Test Loss: 0.245080\n",
            "Processing Sequence Length=3  Done. Train Loss: 0.134670, Test Loss: 0.126162\n",
            "Processing Sequence Length=4  Done. Train Loss: 0.086626, Test Loss: 0.080786\n",
            "Processing Sequence Length=5  Done. Train Loss: 0.064342, Test Loss: 0.060889\n",
            "Processing Sequence Length=6  Done. Train Loss: 0.053400, Test Loss: 0.051467\n",
            "Processing Sequence Length=7  Done. Train Loss: 0.047556, Test Loss: 0.047199\n",
            "Processing Sequence Length=8  Done. Train Loss: 0.045170, Test Loss: 0.045757\n",
            "Processing Sequence Length=9  Done. Train Loss: 0.044354, Test Loss: 0.045730\n",
            "Processing Sequence Length=10  Done. Train Loss: 0.045011, Test Loss: 0.046677\n",
            "Processing Sequence Length=11  Done. Train Loss: 0.046508, Test Loss: 0.048803\n",
            "Processing Sequence Length=12  Done. Train Loss: 0.048583, Test Loss: 0.051330\n",
            "Processing Sequence Length=13  Done. Train Loss: 0.051166, Test Loss: 0.054110\n",
            "Processing Sequence Length=14  Done. Train Loss: 0.053914, Test Loss: 0.057118\n",
            "Processing Sequence Length=15  Done. Train Loss: 0.056923, Test Loss: 0.060115\n",
            "Processing Sequence Length=16  Done. Train Loss: 0.059973, Test Loss: 0.063513\n",
            "Processing Sequence Length=17  Done. Train Loss: 0.063182, Test Loss: 0.066808\n",
            "Processing Sequence Length=18  Done. Train Loss: 0.066473, Test Loss: 0.070702\n",
            "Processing Sequence Length=19  Done. Train Loss: 0.069845, Test Loss: 0.073555\n",
            "Processing Sequence Length=20  Done. Train Loss: 0.073294, Test Loss: 0.078167\n",
            "Processing Sequence Length=21  Done. Train Loss: 0.076719, Test Loss: 0.083368\n",
            "Processing Sequence Length=22  Done. Train Loss: 0.080200, Test Loss: 0.088988\n",
            "Processing Sequence Length=23  Done. Train Loss: 0.083612, Test Loss: 0.094459\n",
            "Processing Sequence Length=24  Done. Train Loss: 0.086964, Test Loss: 0.100119\n",
            "Processing Sequence Length=25  Done. Train Loss: 0.090234, Test Loss: 0.105611\n",
            "Processing Sequence Length=26  Done. Train Loss: 0.093442, Test Loss: 0.111685\n",
            "Processing Sequence Length=27  Done. Train Loss: 0.096536, Test Loss: 0.118164\n",
            "Processing Sequence Length=28  Done. Train Loss: 0.099600, Test Loss: 0.124771\n",
            "Processing Sequence Length=29  Done. Train Loss: 0.102629, Test Loss: 0.131055\n",
            "Processing Sequence Length=30  Done. Train Loss: 0.105650, Test Loss: 0.137638\n",
            "Processing Sequence Length=31  Done. Train Loss: 0.108660, Test Loss: 0.144169\n",
            "Processing Sequence Length=32  Done. Train Loss: 0.111663, Test Loss: 0.151149\n",
            "Processing Sequence Length=33  Done. Train Loss: 0.114637, Test Loss: 0.158324\n",
            "Processing Sequence Length=34  Done. Train Loss: 0.117552, Test Loss: 0.166061\n",
            "Processing Sequence Length=35  Done. Train Loss: 0.120438, Test Loss: 0.173952\n",
            "Processing Sequence Length=36  Done. Train Loss: 0.123282, Test Loss: 0.181304\n",
            "Processing Sequence Length=37  Done. Train Loss: 0.126129, Test Loss: 0.190019\n",
            "Processing Sequence Length=38  Done. Train Loss: 0.128979, Test Loss: 0.198851\n",
            "Processing Sequence Length=39  Done. Train Loss: 0.131845, Test Loss: 0.207541\n",
            "Processing Sequence Length=40  Done. Train Loss: 0.134697, Test Loss: 0.216995\n",
            "Processing Sequence Length=41  Done. Train Loss: 0.137595, Test Loss: 0.226863\n",
            "Processing Sequence Length=42  Done. Train Loss: 0.140490, Test Loss: 0.236413\n",
            "Processing Sequence Length=43  Done. Train Loss: 0.143400, Test Loss: 0.247216\n",
            "Processing Sequence Length=44  Done. Train Loss: 0.146329, Test Loss: 0.258445\n",
            "Processing Sequence Length=45  Done. Train Loss: 0.149220, Test Loss: 0.269846\n",
            "Processing Sequence Length=46  Done. Train Loss: 0.152101, Test Loss: 0.281161\n",
            "Processing Sequence Length=47  Done. Train Loss: 0.154943, Test Loss: 0.291731\n",
            "Processing Sequence Length=48  Done. Train Loss: 0.157759, Test Loss: 0.302487\n",
            "Processing Sequence Length=49  Done. Train Loss: 0.160575, Test Loss: 0.312714\n",
            "Processing Sequence Length=50  Done. Train Loss: 0.163354, Test Loss: 0.323492\n",
            "Processing Sequence Length=51  Done. Train Loss: 0.166126, Test Loss: 0.333901\n",
            "Processing Sequence Length=52  Done. Train Loss: 0.168911, Test Loss: 0.346562\n",
            "Processing Sequence Length=53  Done. Train Loss: 0.171683, Test Loss: 0.359976\n",
            "Processing Sequence Length=54  Done. Train Loss: 0.174440, Test Loss: 0.373123\n",
            "Processing Sequence Length=55  Done. Train Loss: 0.177164, Test Loss: 0.387370\n",
            "Processing Sequence Length=56  Done. Train Loss: 0.179832, Test Loss: 0.402377\n",
            "Processing Sequence Length=57  Done. Train Loss: 0.182475, Test Loss: 0.417514\n",
            "Processing Sequence Length=58  Done. Train Loss: 0.185080, Test Loss: 0.431347\n",
            "Processing Sequence Length=59  Done. Train Loss: 0.187638, Test Loss: 0.445773\n",
            "Processing Sequence Length=60  Done. Train Loss: 0.190168, Test Loss: 0.461117\n",
            "Processing Sequence Length=61  Done. Train Loss: 0.192649, Test Loss: 0.476428\n",
            "Processing Sequence Length=62  Done. Train Loss: 0.195130, Test Loss: 0.490106\n",
            "Processing Sequence Length=63  Done. Train Loss: 0.197573, Test Loss: 0.500328\n",
            "Processing Sequence Length=64  Done. Train Loss: 0.200000, Test Loss: 0.511367\n",
            "Processing Sequence Length=65  Done. Train Loss: 0.202401, Test Loss: 0.522641\n",
            "Processing Sequence Length=66  Done. Train Loss: 0.204769, Test Loss: 0.532855\n",
            "Processing Sequence Length=67  Done. Train Loss: 0.207121, Test Loss: 0.540804\n",
            "Processing Sequence Length=68  Done. Train Loss: 0.209437, Test Loss: 0.549162\n",
            "Processing Sequence Length=69  Done. Train Loss: 0.211755, Test Loss: 0.556769\n",
            "Processing Sequence Length=70  Done. Train Loss: 0.214047, Test Loss: 0.566314\n",
            "Processing Sequence Length=71  Done. Train Loss: 0.216356, Test Loss: 0.571561\n",
            "Processing Sequence Length=72  Done. Train Loss: 0.218623, Test Loss: 0.575602\n",
            "Processing Sequence Length=73  Done. Train Loss: 0.220893, Test Loss: 0.582398\n",
            "Processing Sequence Length=74  Done. Train Loss: 0.223150, Test Loss: 0.592863\n",
            "Processing Sequence Length=75  Done. Train Loss: 0.225393, Test Loss: 0.599726\n",
            "Processing Sequence Length=76  Done. Train Loss: 0.227636, Test Loss: 0.611135\n",
            "Processing Sequence Length=77  Done. Train Loss: 0.229858, Test Loss: 0.618196\n",
            "Processing Sequence Length=78  Done. Train Loss: 0.232047, Test Loss: 0.624741\n",
            "Processing Sequence Length=79  Done. Train Loss: 0.234220, Test Loss: 0.631940\n",
            "Processing Sequence Length=80  Done. Train Loss: 0.236373, Test Loss: 0.639927\n",
            "Processing Sequence Length=81  Done. Train Loss: 0.238502, Test Loss: 0.645158\n",
            "Processing Sequence Length=82  Done. Train Loss: 0.240610, Test Loss: 0.654437\n",
            "Processing Sequence Length=83  Done. Train Loss: 0.242709, Test Loss: 0.663238\n",
            "Processing Sequence Length=84  Done. Train Loss: 0.244803, Test Loss: 0.675766\n",
            "Processing Sequence Length=85  Done. Train Loss: 0.246881, Test Loss: 0.688889\n",
            "Processing Sequence Length=86  Done. Train Loss: 0.248932, Test Loss: 0.698733\n",
            "Processing Sequence Length=87  Done. Train Loss: 0.250954, Test Loss: 0.704978\n",
            "Processing Sequence Length=88  Done. Train Loss: 0.252974, Test Loss: 0.717432\n",
            "Processing Sequence Length=89  Done. Train Loss: 0.254973, Test Loss: 0.734802\n",
            "Processing Sequence Length=90  Done. Train Loss: 0.256962, Test Loss: 0.760056\n",
            "Processing Sequence Length=91  Done. Train Loss: 0.258936, Test Loss: 0.782616\n",
            "Processing Sequence Length=92  Done. Train Loss: 0.260889, Test Loss: 0.799588\n",
            "Processing Sequence Length=93  Done. Train Loss: 0.262830, Test Loss: 0.818482\n",
            "Processing Sequence Length=94  Done. Train Loss: 0.264796, Test Loss: 0.841834\n",
            "Processing Sequence Length=95  Done. Train Loss: 0.266719, Test Loss: 0.867103\n",
            "Processing Sequence Length=96  Done. Train Loss: 0.268618, Test Loss: 0.888950\n",
            "Processing Sequence Length=97  Done. Train Loss: 0.270477, Test Loss: 0.911993\n",
            "Processing Sequence Length=98  Done. Train Loss: 0.272335, Test Loss: 0.930698\n",
            "Processing Sequence Length=99  Done. Train Loss: 0.274181, Test Loss: 0.951560\n",
            "Processing Sequence Length=100  Done. Train Loss: 0.275995, Test Loss: 0.972294\n",
            "Processing Sequence Length=101  Done. Train Loss: 0.277812, Test Loss: 0.993654\n",
            "Processing Sequence Length=102  Done. Train Loss: 0.279613, Test Loss: 1.014813\n",
            "Processing Sequence Length=103  Done. Train Loss: 0.281404, Test Loss: 1.037534\n",
            "Processing Sequence Length=104  Done. Train Loss: 0.283134, Test Loss: 1.060267\n",
            "Processing Sequence Length=105  Done. Train Loss: 0.284836, Test Loss: 1.086269\n",
            "Processing Sequence Length=106  Done. Train Loss: 0.286514, Test Loss: 1.107344\n",
            "Processing Sequence Length=107  Done. Train Loss: 0.288200, Test Loss: 1.130032\n",
            "Processing Sequence Length=108  Done. Train Loss: 0.289844, Test Loss: 1.153502\n",
            "Processing Sequence Length=109  Done. Train Loss: 0.291482, Test Loss: 1.179084\n",
            "Processing Sequence Length=110  Done. Train Loss: 0.293100, Test Loss: 1.202185\n",
            "Processing Sequence Length=111  Done. Train Loss: 0.294711, Test Loss: 1.224884\n",
            "Processing Sequence Length=112  Done. Train Loss: 0.296272, Test Loss: 1.246581\n",
            "Processing Sequence Length=113  Done. Train Loss: 0.297824, Test Loss: 1.270661\n",
            "Processing Sequence Length=114  Done. Train Loss: 0.299383, Test Loss: 1.300022\n",
            "Processing Sequence Length=115  Done. Train Loss: 0.300933, Test Loss: 1.330720\n",
            "Processing Sequence Length=116  Done. Train Loss: 0.302428, Test Loss: 1.373410\n",
            "Processing Sequence Length=117  Done. Train Loss: 0.303912, Test Loss: 1.409092\n",
            "Processing Sequence Length=118  Done. Train Loss: 0.305369, Test Loss: 1.442340\n",
            "Processing Sequence Length=119  Done. Train Loss: 0.306813, Test Loss: 1.478199\n",
            "Processing Sequence Length=120  Done. Train Loss: 0.308221, Test Loss: 1.510707\n",
            "Processing Sequence Length=121  Done. Train Loss: 0.309612, Test Loss: 1.541400\n",
            "Processing Sequence Length=122  Done. Train Loss: 0.311025, Test Loss: 1.571471\n",
            "Processing Sequence Length=123  Done. Train Loss: 0.312381, Test Loss: 1.602390\n",
            "Processing Sequence Length=124  Done. Train Loss: 0.313756, Test Loss: 1.633848\n",
            "Processing Sequence Length=125  Done. Train Loss: 0.315117, Test Loss: 1.660091\n",
            "Processing Sequence Length=126  Done. Train Loss: 0.316449, Test Loss: 1.677504\n",
            "Processing Sequence Length=127  Done. Train Loss: 0.317763, Test Loss: 1.698595\n",
            "Processing Sequence Length=128  Done. Train Loss: 0.319110, Test Loss: 1.715989\n",
            "Processing Sequence Length=129  Done. Train Loss: 0.320435, Test Loss: 1.745994\n",
            "Processing Sequence Length=130  Done. Train Loss: 0.321745, Test Loss: 1.762022\n",
            "Processing Sequence Length=131  Done. Train Loss: 0.323044, Test Loss: 1.773356\n",
            "Processing Sequence Length=132  Done. Train Loss: 0.324306, Test Loss: 1.791460\n",
            "Processing Sequence Length=133  Done. Train Loss: 0.325531, Test Loss: 1.808275\n",
            "Processing Sequence Length=134  Done. Train Loss: 0.326727, Test Loss: 1.832382\n",
            "Processing Sequence Length=135  Done. Train Loss: 0.327917, Test Loss: 1.871205\n",
            "Processing Sequence Length=136  Done. Train Loss: 0.329115, Test Loss: 1.933470\n",
            "Processing Sequence Length=137  Done. Train Loss: 0.330276, Test Loss: 1.980138\n",
            "Processing Sequence Length=138  Done. Train Loss: 0.331438, Test Loss: 2.016179\n",
            "Processing Sequence Length=139  Done. Train Loss: 0.332616, Test Loss: 2.051079\n",
            "Processing Sequence Length=140  Done. Train Loss: 0.333797, Test Loss: 2.066225\n",
            "Processing Sequence Length=141  Done. Train Loss: 0.334964, Test Loss: 2.077050\n",
            "Processing Sequence Length=142  Done. Train Loss: 0.336116, Test Loss: 2.101127\n",
            "Processing Sequence Length=143  Done. Train Loss: 0.337282, Test Loss: 2.128857\n",
            "Processing Sequence Length=144  Done. Train Loss: 0.338463, Test Loss: 2.148216\n",
            "Processing Sequence Length=145  Done. Train Loss: 0.339648, Test Loss: 2.183379\n",
            "Processing Sequence Length=146  Done. Train Loss: 0.340833, Test Loss: 2.228122\n",
            "Processing Sequence Length=147  Done. Train Loss: 0.342018, Test Loss: 2.252600\n",
            "Processing Sequence Length=148  Done. Train Loss: 0.343240, Test Loss: 2.295475\n",
            "Processing Sequence Length=149  Done. Train Loss: 0.344463, Test Loss: 2.321345\n",
            "Processing Sequence Length=150  Done. Train Loss: 0.345684, Test Loss: 2.343918\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Similarly, after completing the calculations, we will plot the results."
      ],
      "metadata": {
        "id": "6K5jjBchyD4u"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zripMcEd3ke",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "bc4404d6-c0e2-4ac7-b2d8-13c752d2e4a3"
      },
      "source": [
        "# Initialize a Plotly figure\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add training data trace\n",
        "fig.add_trace(\n",
        "    go.Scatter(\n",
        "        x=results['sequence_length'],\n",
        "        y=results['train_loss'],\n",
        "        mode='lines+markers',\n",
        "        line=dict(color='red'),\n",
        "        name='Training Loss'\n",
        "    )\n",
        ")\n",
        "\n",
        "# Add testing data trace\n",
        "fig.add_trace(\n",
        "    go.Scatter(\n",
        "        x=results['sequence_length'],\n",
        "        y=results['test_loss'],\n",
        "        mode='lines+markers',\n",
        "        line=dict(color='blue'),\n",
        "        name='Testing Loss'\n",
        "    )\n",
        ")\n",
        "\n",
        "# Update plot layout for better readability and aesthetics\n",
        "fig.update_layout(\n",
        "    title=\"Performance of Linear Regression Model\",\n",
        "    xaxis_title=\"Number of Historical Points (N)\",\n",
        "    yaxis_title=\"Mean Squared Error (MSE)\",\n",
        "    legend_title=\"Dataset Type\",\n",
        "    font=dict(\n",
        "        family=\"Arial, monospace\",\n",
        "        size=18,\n",
        "        color=\"RebeccaPurple\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Display the figure\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"4e2e4714-8ecd-4708-8d98-872b516ba06c\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"4e2e4714-8ecd-4708-8d98-872b516ba06c\")) {                    Plotly.newPlot(                        \"4e2e4714-8ecd-4708-8d98-872b516ba06c\",                        [{\"line\":{\"color\":\"red\"},\"mode\":\"lines+markers\",\"name\":\"Training Loss\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150],\"y\":[0.6288021059062738,0.2565510202019074,0.13467013730297034,0.0866259237369146,0.06434240810370692,0.05339994438317017,0.04755550422633861,0.045169668279248565,0.04435395516204246,0.04501086041806806,0.046508351944655246,0.04858286804063304,0.051165932003655094,0.05391402383571236,0.05692322679273111,0.05997251890368502,0.06318225101800884,0.06647327166750186,0.06984493160002772,0.0732944968879378,0.07671910200500871,0.08020037766593635,0.08361187489186354,0.08696423916041514,0.09023382037734748,0.09344233862782461,0.09653613619035831,0.09959954451060181,0.10262918590781155,0.10564970310231762,0.10866036834779384,0.11166269660770607,0.11463657978081422,0.11755199141090926,0.12043790457780891,0.12328247553621162,0.12612891518810096,0.12897921367981133,0.13184496471038892,0.13469665699102387,0.13759484092578234,0.14049011121632213,0.14340020886465688,0.14632852990861486,0.14922046895863147,0.1521005307053379,0.15494256847777624,0.15775928991064372,0.1605748441373711,0.16335447969621592,0.16612552798903665,0.16891145433323396,0.17168327507045653,0.17443964501304124,0.17716388387629572,0.17983195136195976,0.18247544523888565,0.18508000551060771,0.18763785307197328,0.19016800231976674,0.1926494628443864,0.1951298012819977,0.19757268045723084,0.19999961817313713,0.2024014743078597,0.20476915537480495,0.20712097050340908,0.209437085971768,0.21175470513719322,0.21404667974372513,0.21635573420777074,0.218623487858368,0.22089346399484072,0.22314983090837562,0.2253927838758517,0.22763616696913047,0.22985819258261994,0.23204659477106646,0.23422003841676456,0.23637271013926645,0.23850245458674832,0.24061038083908845,0.24270858042829593,0.244802536045683,0.2468814219583,0.24893199221678677,0.2509543908755288,0.2529741442876832,0.2549734343310896,0.256962053234208,0.25893635756647815,0.2608893915880902,0.2628297178777635,0.2647956985749195,0.266718800969057,0.2686180480885444,0.2704768442634148,0.27233475701861604,0.274181373899157,0.2759947242145922,0.2778116626685825,0.27961320546928825,0.2814038252946158,0.28313364528328216,0.28483573073031426,0.2865140572133138,0.2882004661989406,0.2898435905458932,0.2914816213271876,0.2931001539957484,0.2947112429920406,0.29627212992546276,0.29782378360294703,0.2993833134907744,0.3009332535950085,0.3024279532274206,0.30391191984446314,0.30536921877978607,0.30681323717011033,0.3082210030576114,0.30961168429241553,0.31102505903467287,0.3123813732054386,0.31375604105637833,0.3151168070389088,0.316449099895926,0.3177634355009985,0.3191097651724481,0.32043463195086436,0.32174503562104917,0.3230435286798718,0.3243059844588137,0.3255314749749773,0.3267271883674723,0.327916527657243,0.3291149997132919,0.3302758853650971,0.3314381598781869,0.33261568229985394,0.33379742007015306,0.3349644096634535,0.33611576440649316,0.3372818964541802,0.3384626372508021,0.3396480977522163,0.34083286098533677,0.34201793412420173,0.3432397948671452,0.3444631584513144,0.3456837101410499],\"type\":\"scatter\"},{\"line\":{\"color\":\"blue\"},\"mode\":\"lines+markers\",\"name\":\"Testing Loss\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150],\"y\":[0.6162779104216035,0.24508035097992792,0.1261619807840649,0.08078577767591054,0.06088942911280185,0.051466508281636035,0.04719899165868077,0.04575686249147008,0.04573018869726807,0.0466769581393383,0.04880257729668923,0.05132997695283539,0.0541102687199851,0.05711832939486774,0.060114634988597364,0.06351329298432956,0.06680846547532822,0.07070195664767853,0.07355509624967926,0.07816680840503949,0.08336784181057126,0.08898813420131785,0.09445858968915395,0.10011945094148865,0.10561119769698891,0.11168520215036581,0.11816358262307296,0.12477068128622391,0.13105538439755995,0.13763768983227512,0.14416899988335194,0.15114859959353466,0.15832441611917783,0.166061368258559,0.17395211869990515,0.1813044904751323,0.19001866718595709,0.19885067223449077,0.207540517828378,0.21699466668172207,0.2268625521834874,0.2364125767386639,0.2472160104823046,0.25844502482549886,0.26984581249093553,0.28116053725160095,0.2917308903056095,0.30248722130229394,0.3127138142919169,0.32349197942053465,0.3339008185459974,0.34656192013880005,0.359976299165758,0.3731234086829879,0.3873698348217166,0.4023765042588357,0.4175135552845929,0.4313469843141072,0.4457729139341281,0.461117350256544,0.4764275407584673,0.49010585641045645,0.5003280666008608,0.5113669295818404,0.5226407898901226,0.5328551402354228,0.540803717327874,0.5491623749886785,0.5567687750307228,0.5663143433235698,0.5715610661469409,0.5756020754350283,0.5823976448055089,0.5928625482310419,0.5997264945121873,0.6111352669025276,0.6181961098648637,0.6247409843814838,0.6319400774947364,0.6399270189065543,0.6451577021258915,0.6544373012016169,0.6632384402917499,0.6757663608147911,0.6888893092474743,0.6987330815163194,0.7049777131771803,0.717432396032868,0.7348021242326794,0.7600555436474563,0.7826159867785375,0.7995884361823136,0.8184815502788512,0.8418342154565441,0.8671026777081197,0.8889496173934736,0.9119931207852873,0.9306978044698804,0.951560349829001,0.9722938301634654,0.9936535039687516,1.0148127190738556,1.0375336694242818,1.0602667653163287,1.0862685694194036,1.1073443759371444,1.1300320573927898,1.1535016810293448,1.1790843630042813,1.2021845949036531,1.224884085851347,1.2465813469688172,1.2706609674268916,1.300021518611499,1.3307204944681388,1.3734099100853485,1.4090916245236307,1.4423397697398606,1.4781994060586412,1.5107068453855559,1.5413996227248956,1.5714706950857114,1.6023896465526872,1.6338483777354218,1.6600908836417152,1.677503618691207,1.698595370052975,1.7159891146240942,1.745994334567923,1.7620216455394961,1.7733563117615359,1.7914601990096606,1.8082754129760745,1.8323817820481016,1.8712046032149379,1.933469970381154,1.9801378592017005,2.016179061868812,2.051078905823078,2.0662251714839543,2.0770501848465885,2.101127374894187,2.128857234055134,2.1482158726778597,2.1833785278753264,2.228122459990993,2.252599791855458,2.295474622523725,2.3213449322826873,2.343918242339917],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"font\":{\"family\":\"Arial, monospace\",\"size\":18,\"color\":\"RebeccaPurple\"},\"title\":{\"text\":\"Performance of Linear Regression Model\"},\"xaxis\":{\"title\":{\"text\":\"Number of Historical Points (N)\"}},\"yaxis\":{\"title\":{\"text\":\"Mean Squared Error (MSE)\"}},\"legend\":{\"title\":{\"text\":\"Dataset Type\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('4e2e4714-8ecd-4708-8d98-872b516ba06c');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we will select the best `Sequence Length` and print out the parameters of the model."
      ],
      "metadata": {
        "id": "C7y5m1F9yNAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the equation of the best model\n",
        "losses_test = results['test_loss']\n",
        "minpos = losses_test.index(min(losses_test))\n",
        "print(f'The best sequence length is {minpos + 1} with minimal test loss.')\n",
        "\n",
        "best_model = results['model_coefficients'][minpos]\n",
        "intercept = results['model_intercepts'][minpos]\n",
        "\n",
        "\n",
        "print('The best model is')\n",
        "print('y_pred = ', end='')\n",
        "\n",
        "# Print the intercept\n",
        "print(f'{round(intercept, 3)}', end=\" + \")\n",
        "\n",
        "# Print the coefficients with their respective terms\n",
        "for i, coef in enumerate(best_model):\n",
        "    if i != len(best_model) - 1:\n",
        "        print(f'({round(coef, 3)}*x{i+1})', end=\" + \")\n",
        "        if (i+1) % 5 == 0:\n",
        "            print('\\n\\t', end='')\n",
        "    else:\n",
        "        print(f'({round(coef, 3)}*x{i+1})')"
      ],
      "metadata": {
        "id": "E1zhyvHptVa_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1361cd8b-f3ef-4232-e7fe-f2132f738bf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best sequence length is 9 with minimal test loss.\n",
            "The best model is\n",
            "y_pred = 0.104 + (0.104*x1) + (0.104*x2) + (0.104*x3) + (0.104*x4) + (0.104*x5) + \n",
            "\t(0.104*x6) + (0.104*x7) + (0.104*x8) + (0.104*x9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CwwrvVU8gRtZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}