{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeet1912/ms/blob/main/ds677/assignments/week2/DS677_Week2HW1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYlaRwNu7ojq"
      },
      "source": [
        "# **Week 2 Homework 1: Classification**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Objectives: Solve a classification problem with deep neural networks (DNNs)"
      ],
      "metadata": {
        "id": "A7DRC5V7_8A5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVUGfWTo7_Oj"
      },
      "source": [
        "# Download Data\n",
        "Download data from google drive, then unzip it.\n",
        "\n",
        "You should have\n",
        "- `data/train_split.txt`: training metadata\n",
        "- `data/train_labels`: training labels\n",
        "- `data/test_split.txt`: testing metadata\n",
        "- `data/feat/train/*.pt`: training feature\n",
        "- `data/feat/test/*.pt`:  testing feature\n",
        "\n",
        "after running the following block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzkiMEcC3Foq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8680811-8f63-415c-887c-bc1e0f5f777a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.6)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.8.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1rxGSKq18pO5HbHLx_mTk8NUHM96PxWFs\n",
            "From (redirected): https://drive.google.com/uc?id=1rxGSKq18pO5HbHLx_mTk8NUHM96PxWFs&confirm=t&uuid=5ed8bf4c-609f-47ad-a33d-d5835073d2cd\n",
            "To: /content/data.zip\n",
            "100% 309M/309M [00:02<00:00, 103MB/s] \n",
            "replace __MACOSX/._data? [y]es, [n]o, [A]ll, [N]one, [r]ename: yes\n",
            "replace data/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: yes\n",
            "replace __MACOSX/data/._.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: yes\n",
            "replace data/test_split.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: yes\n",
            "replace __MACOSX/data/._feat? [y]es, [n]o, [A]ll, [N]one, [r]ename: yes\n",
            "replace data/train_label.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: yes\n",
            "replace data/train_split.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: yes\n",
            "replace data/feat/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: yes\n",
            "replace __MACOSX/data/feat/._.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: yes\n",
            "replace __MACOSX/data/feat/._test? [y]es, [n]o, [A]ll, [N]one, [r]ename: yes\n",
            "replace __MACOSX/data/feat/._train? [y]es, [n]o, [A]ll, [N]one, [r]ename: yes\n",
            "replace data/feat/test/id590.pt? [y]es, [n]o, [A]ll, [N]one, [r]ename: yes\n",
            "replace data/feat/test/id601.pt? [y]es, [n]o, [A]ll, [N]one, [r]ename: yes\n",
            "replace data/feat/test/id650.pt? [y]es, [n]o, [A]ll, [N]one, [r]ename: yes\n",
            "replace data/feat/test/id481.pt? [y]es, [n]o, [A]ll, [N]one, [r]ename: yes\n",
            "replace data/feat/test/id532.pt? [y]es, [n]o, [A]ll, [N]one, [r]ename: yes\n",
            "replace data/feat/test/id472.pt? [y]es, [n]o, [A]ll, [N]one, [r]ename: yes\n",
            "replace data/feat/test/id563.pt? [y]es, [n]o, [A]ll, [N]one, [r]ename: yes\n",
            "replace data/feat/test/id423.pt? [y]es, [n]o, [A]ll, [N]one, [r]ename: yes\n",
            "replace data/feat/test/id664.pt? [y]es, [n]o, [A]ll, [N]one, [r]ename: yes\n",
            "replace data/feat/test/id635.pt? [y]es, [n]o, [A]ll, [N]one, [r]ename: yes\n",
            "replace data/feat/test/id417.pt? [y]es, [n]o, [A]ll, [N]one, [r]ename: yes\n",
            "replace data/feat/test/id557.pt? [y]es, [n]o, [A]ll, [N]one, [r]ename: yes\n",
            "replace data/feat/test/id446.pt? [y]es, [n]o, [A]ll, [N]one, [r]ename: yes\n",
            "replace data/feat/test/id506.pt? [y]es, [n]o, [A]ll, [N]one, [r]ename: yes\n",
            "replace data/feat/test/id308.pt? [y]es, [n]o, [A]ll, [N]one, [r]ename: yes\n",
            "replace data/feat/test/id248.pt? [y]es, [n]o, [A]ll, [N]one, [r]ename: yes\n",
            "replace data/feat/test/id79.pt? [y]es, [n]o, [A]ll, [N]one, [r]ename: yes\n",
            "replace data/feat/test/id359.pt? [y]es, [n]o, [A]ll, [N]one, [r]ename: yes\n",
            "replace data/feat/test/id188.pt? [y]es, [n]o, [A]ll, [N]one, [r]ename: yes\n",
            "replace data/feat/test/id219.pt? [y]es, [n]o, [A]ll, [N]one, [r]ename: yes\n",
            "replace data/feat/test/id28.pt? [y]es, [n]o, [A]ll, [N]one, [r]ename: yes\n",
            "replace data/feat/test/id6.pt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A::\n",
            "All\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade gdown\n",
        "\n",
        "# Main link\n",
        "!gdown --id '1rxGSKq18pO5HbHLx_mTk8NUHM96PxWFs' --output data.zip\n",
        "!unzip -q data.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls data"
      ],
      "metadata": {
        "id": "JjO_P_uERwCd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d76e5c30-a28b-46da-9637-9f523f2ba670"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "feat  test_split.txt  train_label.txt  train_split.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some Utility Functions\n",
        "**Fixes random number generator seeds for reproducibility.**"
      ],
      "metadata": {
        "id": "pADUiYODJE1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "\n",
        "def same_seeds(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "BsZKgBZQJjaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_L_4anls8Drv"
      },
      "source": [
        "**Helper functions to pre-process the training data from raw MFCC features of each utterance.**\n",
        "\n",
        "A phoneme may span several frames and is dependent to past and future frames. \\\n",
        "Hence we concatenate neighboring phonemes for training to achieve higher accuracy. The **concat_feat** function concatenates past and future k frames (total 2k+1 = n frames), and we predict the center frame.\n",
        "\n",
        "Feel free to modify the data preprocess functions, but **do not drop any frame** (if you modify the functions, remember to check that the number of frames are the same as mentioned in the slides)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJjLT8em-y9G"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "def load_feat(path):\n",
        "    feat = torch.load(path)\n",
        "    return feat\n",
        "\n",
        "def shift(x, n):\n",
        "    if n < 0:\n",
        "        left = x[0].repeat(-n, 1)\n",
        "        right = x[:n]\n",
        "    elif n > 0:\n",
        "        right = x[-1].repeat(n, 1)\n",
        "        left = x[n:]\n",
        "    else:\n",
        "        return x\n",
        "\n",
        "    return torch.cat((left, right), dim=0)\n",
        "\n",
        "def concat_feat(x, concat_n):\n",
        "    assert concat_n % 2 == 1 # n must be odd\n",
        "    if concat_n < 2:\n",
        "        return x\n",
        "    seq_len, feature_dim = x.size(0), x.size(1)\n",
        "    x = x.repeat(1, concat_n)\n",
        "    x = x.view(seq_len, concat_n, feature_dim).permute(1, 0, 2) # concat_n, seq_len, feature_dim\n",
        "    mid = (concat_n // 2)\n",
        "    for r_idx in range(1, mid+1):\n",
        "        x[mid + r_idx, :] = shift(x[mid + r_idx], r_idx)\n",
        "        x[mid - r_idx, :] = shift(x[mid - r_idx], -r_idx)\n",
        "\n",
        "    return x.permute(1, 0, 2).view(seq_len, concat_n * feature_dim)\n",
        "\n",
        "def preprocess_data(split, feat_dir, phone_path, concat_nframes, train_ratio=0.8, random_seed=1213):\n",
        "    class_num = 41 # NOTE: pre-computed, should not need change\n",
        "\n",
        "    if split == 'train' or split == 'val':\n",
        "        mode = 'train'\n",
        "    elif split == 'test':\n",
        "        mode = 'test'\n",
        "    else:\n",
        "        raise ValueError('Invalid \\'split\\' argument for dataset: PhoneDataset!')\n",
        "\n",
        "    label_dict = {}\n",
        "    if mode == 'train':\n",
        "        for line in open(os.path.join(phone_path, f'{mode}_label.txt')).readlines():\n",
        "            line = line.strip('\\n').split(' ')\n",
        "            label_dict[line[0]] = [int(p) for p in line[1:]]\n",
        "\n",
        "        # split training and validation data\n",
        "        usage_list = open(os.path.join(phone_path, 'train_split.txt')).readlines()\n",
        "        random.seed(random_seed)\n",
        "        random.shuffle(usage_list)\n",
        "        train_len = int(len(usage_list) * train_ratio)\n",
        "        usage_list = usage_list[:train_len] if split == 'train' else usage_list[train_len:]\n",
        "\n",
        "    elif mode == 'test':\n",
        "        usage_list = open(os.path.join(phone_path, 'test_split.txt')).readlines()\n",
        "\n",
        "    usage_list = [line.strip('\\n') for line in usage_list]\n",
        "    print('[Dataset] - # phone classes: ' + str(class_num) + ', number of utterances for ' + split + ': ' + str(len(usage_list)))\n",
        "\n",
        "    max_len = 3000000\n",
        "    X = torch.empty(max_len, 39 * concat_nframes)\n",
        "    if mode == 'train':\n",
        "        y = torch.empty(max_len, dtype=torch.long)\n",
        "\n",
        "    idx = 0\n",
        "    for i, fname in tqdm(enumerate(usage_list)):\n",
        "        feat = load_feat(os.path.join(feat_dir, mode, f'{fname}.pt'))\n",
        "        cur_len = len(feat)\n",
        "        feat = concat_feat(feat, concat_nframes)\n",
        "        if mode == 'train':\n",
        "          label = torch.LongTensor(label_dict[fname])\n",
        "\n",
        "        X[idx: idx + cur_len, :] = feat\n",
        "        if mode == 'train':\n",
        "          y[idx: idx + cur_len] = label\n",
        "\n",
        "        idx += cur_len\n",
        "\n",
        "    X = X[:idx, :]\n",
        "    if mode == 'train':\n",
        "      y = y[:idx]\n",
        "\n",
        "    print(f'[INFO] {split} set')\n",
        "    print(X.shape)\n",
        "    if mode == 'train':\n",
        "      print(y.shape)\n",
        "      return X, y\n",
        "    else:\n",
        "      return X\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us5XW_x6udZQ"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fjf5EcmJtf4e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SpeechDataset(Dataset):\n",
        "    def __init__(self, X, y=None):\n",
        "        self.data = X\n",
        "        if y is not None:\n",
        "            self.label = torch.LongTensor(y)\n",
        "        else:\n",
        "            self.label = None\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.label is not None:\n",
        "            return self.data[idx], self.label[idx]\n",
        "        else:\n",
        "            return self.data[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRqKNvNZwe3V"
      },
      "source": [
        "# Model\n",
        "Feel free to modify the structure of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bg-GRd7ywdrL"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(BasicBlock, self).__init__()\n",
        "\n",
        "        # TODO: apply batch normalization and dropout for strong baseline.\n",
        "        # Reference: https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html (batch normalization)\n",
        "        #       https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html (dropout)\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Linear(input_dim, output_dim),\n",
        "            nn.BatchNorm1d(output_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.4)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block(x)\n",
        "        return x\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim=41, hidden_layers=3, hidden_dim=512):\n",
        "        super(Classifier, self).__init__()\n",
        "\n",
        "        layers = [BasicBlock(input_dim, hidden_dim)]\n",
        "        for _ in range(hidden_layers):\n",
        "            layers.append(BasicBlock(hidden_dim, hidden_dim))\n",
        "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
        "        self.fc = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyper-parameters"
      ],
      "metadata": {
        "id": "TlIq8JeqvvHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data prarameters\n",
        "# TODO: change the value of \"concat_nframes\" for medium baseline\n",
        "concat_nframes = 11   # the number of frames to concat with, n must be odd (total 2k+1 = n frames)\n",
        "train_ratio = 0.80   # the ratio of data used for training, the rest will be used for validation\n",
        "\n",
        "# training parameters\n",
        "seed = 17          # random seed\n",
        "batch_size = 256        # batch size\n",
        "num_epoch = 50         # the number of training epoch\n",
        "learning_rate = 1e-5      # learning rate\n",
        "model_path = './model.ckpt'  # the path where the checkpoint will be saved\n",
        "\n",
        "# model parameters\n",
        "# TODO: change the value of \"hidden_layers\" or \"hidden_dim\" for medium baseline\n",
        "input_dim = 39 * concat_nframes  # the input dim of the model, you should not change the value\n",
        "hidden_layers = 9          # the number of hidden layers\n",
        "hidden_dim = 2048           # the hidden dim"
      ],
      "metadata": {
        "id": "iIHn79Iav1ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataloader"
      ],
      "metadata": {
        "id": "IIUFRgG5yoDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import gc\n",
        "\n",
        "same_seeds(seed)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'DEVICE: {device}')\n",
        "\n",
        "# preprocess data\n",
        "train_X, train_y = preprocess_data(split='train', feat_dir='./data/feat', phone_path='./data', concat_nframes=concat_nframes, train_ratio=train_ratio, random_seed=seed)\n",
        "val_X, val_y = preprocess_data(split='val', feat_dir='./data/feat', phone_path='./data', concat_nframes=concat_nframes, train_ratio=train_ratio, random_seed=seed)\n",
        "\n",
        "# get dataset\n",
        "train_set = SpeechDataset(train_X, train_y)\n",
        "val_set = SpeechDataset(val_X, val_y)\n",
        "\n",
        "# remove raw feature to save memory\n",
        "del train_X, train_y, val_X, val_y\n",
        "gc.collect()\n",
        "\n",
        "# get dataloader\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "c1zI3v5jyrDn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "069e3288-7848-484f-90d4-c697e432311c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEVICE: cuda\n",
            "[Dataset] - # phone classes: 41, number of utterances for train: 2194\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]<ipython-input-79-0638e8ccf030>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  feat = torch.load(path)\n",
            "2194it [00:04, 466.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] train set\n",
            "torch.Size([1356391, 429])\n",
            "torch.Size([1356391])\n",
            "[Dataset] - # phone classes: 41, number of utterances for val: 549\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "549it [00:01, 494.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] val set\n",
            "torch.Size([340372, 429])\n",
            "torch.Size([340372])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "pwWH1KIqzxEr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdMWsBs7zzNs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da9452c6-c181-4412-f205-5157d62f58bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5299/5299 [02:41<00:00, 32.83it/s]\n",
            "100%|██████████| 1330/1330 [00:10<00:00, 128.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[001/050] Train Acc: 0.30192 Loss: 2.55462 | Val Acc: 0.33776 loss: 3.02945\n",
            "saving model with acc 0.33776\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5299/5299 [02:39<00:00, 33.25it/s]\n",
            "100%|██████████| 1330/1330 [00:10<00:00, 128.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[002/050] Train Acc: 0.43350 Loss: 1.93686 | Val Acc: 0.47797 loss: 1.85520\n",
            "saving model with acc 0.47797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5299/5299 [02:39<00:00, 33.26it/s]\n",
            "100%|██████████| 1330/1330 [00:10<00:00, 128.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[003/050] Train Acc: 0.48322 Loss: 1.73688 | Val Acc: 0.53118 loss: 1.59572\n",
            "saving model with acc 0.53118\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5299/5299 [02:39<00:00, 33.24it/s]\n",
            "100%|██████████| 1330/1330 [00:10<00:00, 127.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[004/050] Train Acc: 0.51141 Loss: 1.62581 | Val Acc: 0.55732 loss: 1.46856\n",
            "saving model with acc 0.55732\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5299/5299 [02:39<00:00, 33.21it/s]\n",
            "100%|██████████| 1330/1330 [00:10<00:00, 126.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[005/050] Train Acc: 0.53096 Loss: 1.55043 | Val Acc: 0.57553 loss: 1.39618\n",
            "saving model with acc 0.57553\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5299/5299 [02:39<00:00, 33.24it/s]\n",
            "100%|██████████| 1330/1330 [00:10<00:00, 127.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[006/050] Train Acc: 0.54604 Loss: 1.49439 | Val Acc: 0.58924 loss: 1.34346\n",
            "saving model with acc 0.58924\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5299/5299 [02:39<00:00, 33.22it/s]\n",
            "100%|██████████| 1330/1330 [00:10<00:00, 128.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[007/050] Train Acc: 0.55744 Loss: 1.45358 | Val Acc: 0.59996 loss: 1.30843\n",
            "saving model with acc 0.59996\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5299/5299 [02:39<00:00, 33.22it/s]\n",
            "100%|██████████| 1330/1330 [00:10<00:00, 128.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[008/050] Train Acc: 0.56739 Loss: 1.41757 | Val Acc: 0.60912 loss: 1.27008\n",
            "saving model with acc 0.60912\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5299/5299 [02:39<00:00, 33.20it/s]\n",
            "100%|██████████| 1330/1330 [00:10<00:00, 128.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[009/050] Train Acc: 0.57539 Loss: 1.38791 | Val Acc: 0.61526 loss: 1.24569\n",
            "saving model with acc 0.61526\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5299/5299 [02:39<00:00, 33.22it/s]\n",
            "100%|██████████| 1330/1330 [00:10<00:00, 126.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[010/050] Train Acc: 0.58192 Loss: 1.36280 | Val Acc: 0.62190 loss: 1.22218\n",
            "saving model with acc 0.62190\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5299/5299 [02:39<00:00, 33.24it/s]\n",
            "100%|██████████| 1330/1330 [00:10<00:00, 126.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[011/050] Train Acc: 0.58814 Loss: 1.34118 | Val Acc: 0.62741 loss: 1.20049\n",
            "saving model with acc 0.62741\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5299/5299 [02:39<00:00, 33.24it/s]\n",
            "100%|██████████| 1330/1330 [00:10<00:00, 128.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[012/050] Train Acc: 0.59290 Loss: 1.32259 | Val Acc: 0.63271 loss: 1.17978\n",
            "saving model with acc 0.63271\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5299/5299 [02:39<00:00, 33.20it/s]\n",
            "100%|██████████| 1330/1330 [00:10<00:00, 127.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[013/050] Train Acc: 0.59803 Loss: 1.30474 | Val Acc: 0.63580 loss: 1.16790\n",
            "saving model with acc 0.63580\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5299/5299 [02:40<00:00, 33.07it/s]\n",
            "100%|██████████| 1330/1330 [00:10<00:00, 129.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[014/050] Train Acc: 0.60260 Loss: 1.28791 | Val Acc: 0.63993 loss: 1.15800\n",
            "saving model with acc 0.63993\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5299/5299 [02:40<00:00, 33.04it/s]\n",
            "100%|██████████| 1330/1330 [00:10<00:00, 126.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[015/050] Train Acc: 0.60661 Loss: 1.27324 | Val Acc: 0.64373 loss: 1.14206\n",
            "saving model with acc 0.64373\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 62%|██████▏   | 3296/5299 [01:39<01:01, 32.75it/s]"
          ]
        }
      ],
      "source": [
        "# create model, define a loss function, and optimizer\n",
        "model = Classifier(input_dim=input_dim, hidden_layers=hidden_layers, hidden_dim=hidden_dim).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "best_acc = 0.0\n",
        "for epoch in range(num_epoch):\n",
        "    train_acc = 0.0\n",
        "    train_loss = 0.0\n",
        "    val_acc = 0.0\n",
        "    val_loss = 0.0\n",
        "\n",
        "    # training\n",
        "    model.train() # set the model to training mode\n",
        "    for i, batch in enumerate(tqdm(train_loader)):\n",
        "        features, labels = batch\n",
        "        features = features.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, train_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
        "        train_acc += (train_pred.detach() == labels.detach()).sum().item()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # validation\n",
        "    model.eval() # set the model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(tqdm(val_loader)):\n",
        "            features, labels = batch\n",
        "            features = features.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(features)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            _, val_pred = torch.max(outputs, 1)\n",
        "            val_acc += (val_pred.cpu() == labels.cpu()).sum().item() # get the index of the class with the highest probability\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    print(f'[{epoch+1:03d}/{num_epoch:03d}] Train Acc: {train_acc/len(train_set):3.5f} Loss: {train_loss/len(train_loader):3.5f} | Val Acc: {val_acc/len(val_set):3.5f} loss: {val_loss/len(val_loader):3.5f}')\n",
        "\n",
        "    # if the model improves, save a checkpoint at this epoch\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "        print(f'saving model with acc {best_acc/len(val_set):.5f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab33MxosWLmG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3c3b61b-249d-40ce-b7e1-41e1711760ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "del train_set, val_set\n",
        "del train_loader, val_loader\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hi7jTn3PX-m"
      },
      "source": [
        "# Testing\n",
        "Create a testing dataset, and load model from the saved checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOG1Ou0PGrhc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86e148f5-46bb-4c6e-84c7-813a65b4f057"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Dataset] - # phone classes: 41, number of utterances for test: 686\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]<ipython-input-4-0638e8ccf030>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  feat = torch.load(path)\n",
            "686it [00:01, 478.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] test set\n",
            "torch.Size([420031, 429])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# load data\n",
        "test_X = preprocess_data(split='test', feat_dir='./data/feat', phone_path='./data', concat_nframes=concat_nframes)\n",
        "test_set = SpeechDataset(test_X, None)\n",
        "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ay0Fu8Ovkdad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac87ae73-4246-404a-9cb3-85031f8f5bdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-73-f7b7612de35f>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "# load model\n",
        "model = Classifier(input_dim=input_dim, hidden_layers=hidden_layers, hidden_dim=hidden_dim).to(device)\n",
        "model.load_state_dict(torch.load(model_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zp-DV1p4r7Nz"
      },
      "source": [
        "Make prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84HU5GGjPqR0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93670cdc-3cc8-4219-c0d6-7674d34df368"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1641/1641 [00:12<00:00, 134.32it/s]\n"
          ]
        }
      ],
      "source": [
        "pred = np.array([], dtype=np.int32)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for i, batch in enumerate(tqdm(test_loader)):\n",
        "        features = batch\n",
        "        features = features.to(device)\n",
        "\n",
        "        outputs = model(features)\n",
        "\n",
        "        _, test_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
        "        pred = np.concatenate((pred, test_pred.cpu().numpy()), axis=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyZqy40Prz0v"
      },
      "source": [
        "Write prediction to a CSV file.\n",
        "\n",
        "After finish running this block, download the file `prediction.csv` from the files section on the left-hand side and submit it to Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuljYSPHcZir"
      },
      "outputs": [],
      "source": [
        "with open('prediction.csv', 'w') as f:\n",
        "    f.write('Id,Label\\n')\n",
        "    for i, y in enumerate(pred):\n",
        "        f.write('{},{}\\n'.format(i, y))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jqsWNhADbuhb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}