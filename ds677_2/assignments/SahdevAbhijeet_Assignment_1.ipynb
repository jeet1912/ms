{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tajfsk_7JY3E"
      },
      "source": [
        "**Note to grader:** Each question consists of parts, e.g. Q1(i), Q1(ii), etc. Each part must be first graded  on a 0-4 scale, following the standard NJIT convention (A:4, B+: 3.5, B:3, C+: 2.5, C: 2, D:1, F:0). However, any given item may be worth 4 or 8 points; if an item is worth 8 points, you need to accordingly scale the 0-4 grade.\n",
        "\n",
        "\n",
        "The total score must be re-scaled to 100. That should apply to all future assignments so that Canvas assigns the same weight on all assignments.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SArgW_Vq-uTh"
      },
      "source": [
        "# **Assignment 1**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-CQn16CadJL"
      },
      "source": [
        "This assignment familiarizes you with Pytorch and reinforces the key steps in defining and training a simple regression model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlFM4hig-uTj"
      },
      "source": [
        "## Preparation Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "E3alYkjM-uTk"
      },
      "outputs": [],
      "source": [
        "# Import all necessary python packages\n",
        "import numpy as np\n",
        "import torch\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfrfDK0P-uT5"
      },
      "source": [
        "## <font color = 'blue'> **Question 1. Basic Operations with Tensors** </font>\n",
        "\n",
        "Your task for this question is to follow the NumPy  [**tutorial**](https://jalammar.github.io/visual-numpy/?fbclid=IwAR0tSntx5mj1aHteokRKrT4G6z77M3z0Quj40AQZ9mvKlhs2RTN3xXrc6Eo) (up to section **Data Representations**) and 'mirror' each of the operations presented in the tutorial with tensors in PyTorch.\n",
        "\n",
        "You may find useful to consult this PyTorch introductory [tutorial](https://jhui.github.io/2018/02/09/PyTorch-Basic-operations/), and as always the full PyTorch [documentation](https://pytorch.org/docs/stable/torch.html) is the ultimate resource.\n",
        "\n",
        "*(Please insert cells below for your answers )*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x1126d2510>"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.manual_seed(33)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "4po5m-tq-uT6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "tensor([1., 2., 3.])\n",
            "tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]])\n",
            "tensor([[0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.]])\n",
            "tensor([[0.6186, 0.5587, 0.1937, 0.3360],\n",
            "        [0.2008, 0.6970, 0.6367, 0.1452],\n",
            "        [0.8213, 0.2365, 0.3702, 0.9168],\n",
            "        [0.0333, 0.7183, 0.0325, 0.2320]])\n",
            "tensor([[1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.]])\n",
            "tensor([[-0.4415, -1.5009,  1.2530, -0.6902],\n",
            "        [ 0.2395,  1.3365, -0.5728,  0.5368],\n",
            "        [ 0.4898, -1.2608, -0.3192,  0.3207],\n",
            "        [-0.1966, -0.7767, -1.4657, -1.0870]])\n",
            "tensor([0, 3, 1, 2])\n",
            "tensor([[1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.]], dtype=torch.float16)\n"
          ]
        }
      ],
      "source": [
        "x = torch.Tensor(3,3)\n",
        "y = torch.Tensor([1,2,3])\n",
        "z = torch.Tensor([[1,2,3],[4,5,6]])\n",
        "a = torch.zeros(4,4)\n",
        "b = torch.rand(4,4)\n",
        "c = torch.ones(4,4)\n",
        "r1 = torch.randn(4,4).type(torch.FloatTensor) # random numbers with SD = 1 and mean = 0\n",
        "r2 = torch.randperm(4)\n",
        "r3 = torch.ones([2,4], dtype=torch.float16)\n",
        "\n",
        "print(x)\n",
        "print(y)\n",
        "print(z)\n",
        "print(a)\n",
        "print(b)\n",
        "print(c)\n",
        "print(r1)\n",
        "print(r2)\n",
        "print(r3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1., 0., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 0., 0., 1.]])\n",
            "tensor([[[[1., 1.],\n",
            "          [1., 1.]],\n",
            "\n",
            "         [[1., 1.],\n",
            "          [1., 1.]]],\n",
            "\n",
            "\n",
            "        [[[1., 1.],\n",
            "          [1., 1.]],\n",
            "\n",
            "         [[1., 1.],\n",
            "          [1., 1.]]]])\n",
            "tensor([[1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.]])\n",
            "tensor([0, 1, 2, 3, 4])\n"
          ]
        }
      ],
      "source": [
        "i = torch.eye(4) #identity matrix\n",
        "v = torch.ones(2, 2, 2, 2)\n",
        "v1 = torch.ones_like(i)\n",
        "v2 = torch.arange(0,5,1)\n",
        "print(i)\n",
        "print(v)\n",
        "print(v1)\n",
        "print(v2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Arithmetic (element-wise)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1., 2., 3.],\n",
            "        [1., 2., 3.],\n",
            "        [1., 2., 3.]])\n"
          ]
        }
      ],
      "source": [
        "x.add_(y)\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "d = torch.sub(x,y)\n",
        "print(d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1., 4., 9.],\n",
            "        [1., 4., 9.],\n",
            "        [1., 4., 9.]])\n"
          ]
        }
      ],
      "source": [
        "e = torch.Tensor(3,3)\n",
        "torch.mul(x,y,out=e)\n",
        "print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]])\n"
          ]
        }
      ],
      "source": [
        "f = torch.div(x,y)\n",
        "print(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[10., 10., 10.],\n",
            "        [10., 10., 10.],\n",
            "        [10., 10., 10.]])\n"
          ]
        }
      ],
      "source": [
        "f.mul_(10)\n",
        "print(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Indexing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1., 1., 1.])\n",
            "tensor([1., 4., 9.])\n",
            "tensor([[4., 9.],\n",
            "        [4., 9.],\n",
            "        [4., 9.]])\n",
            "tensor(9.)\n"
          ]
        }
      ],
      "source": [
        "print(e[:,0])\n",
        "print(e[0,:])\n",
        "print(e[0:3,1:3])\n",
        "print(e[0,2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Split a tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(tensor([[1., 4., 9.]]), tensor([[1., 4., 9.]]), tensor([[1., 4., 9.]]))\n"
          ]
        }
      ],
      "source": [
        "r = torch.chunk(e,3)\n",
        "print(r)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(tensor([[-0.4415, -1.5009,  1.2530, -0.6902],\n",
            "        [ 0.2395,  1.3365, -0.5728,  0.5368]]), tensor([[ 0.4898, -1.2608, -0.3192,  0.3207],\n",
            "        [-0.1966, -0.7767, -1.4657, -1.0870]]))\n"
          ]
        }
      ],
      "source": [
        "r = torch.split(r1,2)\n",
        "print(r)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Reshape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.4415, -1.5009],\n",
            "        [ 1.2530, -0.6902],\n",
            "        [ 0.2395,  1.3365],\n",
            "        [-0.5728,  0.5368],\n",
            "        [ 0.4898, -1.2608],\n",
            "        [-0.3192,  0.3207],\n",
            "        [-0.1966, -0.7767],\n",
            "        [-1.4657, -1.0870]])\n"
          ]
        }
      ],
      "source": [
        "r4 = r1.view(-1,2)\n",
        "print(r4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([9, 1])\n"
          ]
        }
      ],
      "source": [
        "E = e.reshape(-1,1)\n",
        "print(E.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 3, 4])\n",
            "torch.Size([3, 4])\n"
          ]
        }
      ],
      "source": [
        "X = torch.randn(3, 4)       \n",
        "Y = X.unsqueeze(0)         \n",
        "print(Y.shape)  \n",
        "\n",
        "Z = Y.squeeze(0)\n",
        "print(Z.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Transpose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.4415,  0.2395,  0.4898, -0.1966],\n",
            "        [-1.5009,  1.3365, -1.2608, -0.7767],\n",
            "        [ 1.2530, -0.5728, -0.3192, -1.4657],\n",
            "        [-0.6902,  0.5368,  0.3207, -1.0870]])\n"
          ]
        }
      ],
      "source": [
        "r1_t = r1.t()\n",
        "print(r1_t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 2, 4])\n"
          ]
        }
      ],
      "source": [
        "r5 = torch.randn(2,3,4)\n",
        "r5_t = r5.transpose(0,1)\n",
        "print(r5_t.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Dot Product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(14.)\n"
          ]
        }
      ],
      "source": [
        "r6 = torch.dot(y,y) #1d tensors only\n",
        "print(r6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Matrix - Vector Product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1.4434, 0.6357, 0.0170])\n"
          ]
        }
      ],
      "source": [
        "mat = torch.randn(3,4)\n",
        "vec = torch.rand(4)\n",
        "res = torch.mv(mat,vec)\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1.6289, 1.0830, 0.5203])\n"
          ]
        }
      ],
      "source": [
        "# Matrix + Mat*vec\n",
        "M = torch.rand(3)\n",
        "res = torch.addmv(M,mat,vec)\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Matrix - Matrix Product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[2.5321, 0.5235],\n",
            "        [0.5326, 0.1301],\n",
            "        [1.9316, 0.7628]])\n"
          ]
        }
      ],
      "source": [
        "m = torch.randn(3,3)\n",
        "n = torch.randn(3,2)\n",
        "res = torch.mm(m,n)\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 1.9501,  1.0562],\n",
            "        [ 1.7942, -1.3221],\n",
            "        [ 1.9689, -0.3867]])\n"
          ]
        }
      ],
      "source": [
        "# Matrix + Matrix*Matrix\n",
        "M = torch.randn(3,2)\n",
        "res = torch.addmm(M,m,n)\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Batch Matrix Multiplication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([10, 3, 5])\n",
            "tensor([[[ 1.8225,  1.4701,  1.9491,  1.0586, -1.0528],\n",
            "         [ 0.4583,  1.3312,  1.6611,  0.4987,  1.3900],\n",
            "         [-1.6847,  1.4925, -1.0693, -1.7511, -2.0955]],\n",
            "\n",
            "        [[-0.3649, -0.9117,  3.0273,  2.9765, -2.0111],\n",
            "         [ 0.7599,  2.6158,  1.3514, -1.3128,  0.8260],\n",
            "         [ 0.3869,  0.6798,  3.4102,  1.9362, -2.1905]],\n",
            "\n",
            "        [[-2.0007, -1.4244, -2.4081,  0.9214, -0.1149],\n",
            "         [-1.0363, -0.2101,  0.7530, -3.8958, -1.8063],\n",
            "         [-1.1981, -3.4992,  1.2785,  0.6312, -0.2503]],\n",
            "\n",
            "        [[-4.2638, -1.6138,  2.0528, -1.8974, -1.2153],\n",
            "         [ 6.4677, -1.9592, -0.8187,  4.9601,  4.9604],\n",
            "         [-0.3094,  2.0407,  2.4481, -0.1072,  0.9820]],\n",
            "\n",
            "        [[ 1.5655,  0.8277, -0.7178,  1.8265,  0.3514],\n",
            "         [-0.1505,  1.6630,  0.6531,  1.3612,  0.9814],\n",
            "         [ 4.8763,  0.0597, -3.8884,  2.5301, -1.7111]],\n",
            "\n",
            "        [[ 2.4039,  0.2413, -0.3492,  0.5997,  1.3322],\n",
            "         [-3.8226, -1.9117,  0.3516,  0.1737, -2.1312],\n",
            "         [ 0.1729,  1.5640, -0.4557,  0.2757,  0.1393]],\n",
            "\n",
            "        [[ 1.8880,  0.2892,  1.0945, -1.5096, -1.9474],\n",
            "         [ 2.5383, -2.7254, -1.4616, -0.1382, -1.8997],\n",
            "         [-0.0081, -2.5897, -4.7992, -1.9461,  0.8096]],\n",
            "\n",
            "        [[-0.6062, -0.0112,  0.5412, -0.3127,  0.5733],\n",
            "         [-0.1387,  1.3797,  2.3112, -1.0528, -1.6090],\n",
            "         [-0.9790, -1.3957,  2.3792, -0.4396, -1.3297]],\n",
            "\n",
            "        [[ 0.1399, -0.4829,  1.2509, -0.7550, -1.0598],\n",
            "         [-0.6106, -2.6691, -1.2393, -0.2562, -0.3696],\n",
            "         [ 1.6779, -3.0075, -1.5792,  1.9670,  1.3553]],\n",
            "\n",
            "        [[-1.9338, -0.4238, -0.4274, -1.4974, -2.7945],\n",
            "         [ 0.6682, -2.3743, -1.2726, -2.8511,  0.8064],\n",
            "         [-0.0364,  0.9511, -0.1490,  0.0170, -0.0493]]])\n"
          ]
        }
      ],
      "source": [
        "A = torch.randn(10, 3, 4)  # batch of 10\n",
        "B = torch.randn(10, 4, 5)  \n",
        "C = torch.bmm(A, B)        \n",
        "print(C.shape)\n",
        "print(C)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.3365)\n"
          ]
        }
      ],
      "source": [
        "largest = torch.max(r1)\n",
        "print(largest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(-1.5009)\n"
          ]
        }
      ],
      "source": [
        "smallest = torch.min(r1)\n",
        "print(smallest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(-0.2584)\n"
          ]
        }
      ],
      "source": [
        "mean = torch.mean(r1)\n",
        "print(mean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([-0.3449,  0.3850, -0.1924, -0.8815])\n",
            "tensor([ 0.0228, -0.5505, -0.2762, -0.2299])\n"
          ]
        }
      ],
      "source": [
        "rowMeans = torch.mean(r1,dim=1)\n",
        "print(rowMeans)\n",
        "colMeans = torch.mean(r1,dim=0)\n",
        "print(colMeans)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Mean Squared Error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MSE is  tensor(0.1495)\n"
          ]
        }
      ],
      "source": [
        "pred = torch.rand(10)\n",
        "obs = torch.rand(10)\n",
        "mse = torch.mean((pred-obs)**2)\n",
        "print('MSE is ',mse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IllLoXxGAIIo"
      },
      "outputs": [],
      "source": [
        "# For grader use only\n",
        "\n",
        "G = [0]*2\n",
        "\n",
        "\n",
        "# insert grade here  (from 0 to 8)\n",
        "# G[1] =\n",
        "\n",
        "# please justify point subtraction  s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0oU4P0jcTUP"
      },
      "source": [
        "## <font color = 'blue'> **Question 2. Using the GPU** </font>\n",
        "\n",
        "(i) Initialize randomly two 2D tensors, of dimensions 5000 x 5000 using torch.randn(). Multiply them 100 times on the CPU, and measure the total runtime. <br>\n",
        "(ii) Activate and detect the GPU. Move the tensors to the GPU and repeat the above experiment. What do you observe? <br>\n",
        "(iii) Continuing on the GPU, measure the runtime of a loop that does the following 1000 times:\n",
        "*   (a) Define two random 100x100 matrices, A and B\n",
        "*   (b) Multiply A and B\n",
        "*   (c) Print the Frobenius norm of the product\n",
        "\n",
        "\n",
        "(iv) Now do a very similar experiment. Measure the runtime of a loop that does the following 1000 times:\n",
        "*   (a) Define two random 100x100 matrices, A and B\n",
        "*   (b) Multiply A and B\n",
        "*   (c) Store the Frobenius norm of the product in a 1D tensor $r$  that has been placed in the GPU. <br>\n",
        "Finally, when exiting the loop, print the content of the tensor $r$. What do you observe, and how do you explain it?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "t1 = torch.randn(5000,5000)\n",
        "t2 = torch.randn(5000,5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total time on CPU is 9.72259831 seconds\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "start = time.time()\n",
        "for _ in range(100):\n",
        "    res = torch.mm(t1,t2)\n",
        "end = time.time()\n",
        "time1 = end - start\n",
        "print(f'Total time on CPU is {time1:.8f} seconds')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using MPS (Apple GPU)\n"
          ]
        }
      ],
      "source": [
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device('mps')\n",
        "    print('Using MPS (Apple GPU)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total time on mps is 3.00121808 seconds\n"
          ]
        }
      ],
      "source": [
        "t1 = t1.to(device)\n",
        "t2 = t2.to(device)\n",
        "\n",
        "torch.mps.synchronize() #ensures previous gpu work is done\n",
        "start = time.time()\n",
        "for _ in range(100):\n",
        "    res = torch.mm(t1,t2)\n",
        "torch.mps.synchronize() #waits for gpu work to finish\n",
        "end = time.time()\n",
        "time2 = end - start\n",
        "print(f'Total time on {device} is {time2:.8f} seconds')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performance boost : 3.2395507602028943x\n",
            "Reduction Percentage : 69.13152242327052%\n"
          ]
        }
      ],
      "source": [
        "perfBoost = time1 / time2 \n",
        "print(f'Performance boost : {perfBoost}x')\n",
        "\n",
        "reductionPerc = (time1-time2)/time1 * 100\n",
        "print(f'Reduction Percentage : {reductionPerc}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### ii) Observations \n",
        "A performance boost, i.e, $cpuTime/mpsTime$ of <b>3.23x</b> with a relative reduction in computation time, i.e, $(cpuTime - mpsTime) / cpuTime$ of <b>69.13%</b>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Frobenius norm: 2482.5205078125\n",
            "Frobenius norm: 2485.42041015625\n",
            "Frobenius norm: 2528.246826171875\n",
            "Frobenius norm: 2498.389404296875\n",
            "Frobenius norm: 2518.50439453125\n",
            "Frobenius norm: 2543.437744140625\n",
            "Frobenius norm: 2498.574951171875\n",
            "Frobenius norm: 2546.34033203125\n",
            "Frobenius norm: 2517.173095703125\n",
            "Frobenius norm: 2523.905029296875\n",
            "Frobenius norm: 2490.16748046875\n",
            "Frobenius norm: 2472.15380859375\n",
            "Frobenius norm: 2516.241455078125\n",
            "Frobenius norm: 2507.14404296875\n",
            "Frobenius norm: 2512.6513671875\n",
            "Frobenius norm: 2489.712646484375\n",
            "Frobenius norm: 2496.268310546875\n",
            "Frobenius norm: 2486.9794921875\n",
            "Frobenius norm: 2511.893310546875\n",
            "Frobenius norm: 2484.94677734375\n",
            "Frobenius norm: 2491.867431640625\n",
            "Frobenius norm: 2507.839111328125\n",
            "Frobenius norm: 2540.99609375\n",
            "Frobenius norm: 2552.773193359375\n",
            "Frobenius norm: 2517.564697265625\n",
            "Frobenius norm: 2505.604248046875\n",
            "Frobenius norm: 2544.404052734375\n",
            "Frobenius norm: 2507.220703125\n",
            "Frobenius norm: 2496.630126953125\n",
            "Frobenius norm: 2516.93603515625\n",
            "Frobenius norm: 2495.5849609375\n",
            "Frobenius norm: 2520.3046875\n",
            "Frobenius norm: 2506.45458984375\n",
            "Frobenius norm: 2483.147216796875\n",
            "Frobenius norm: 2522.7548828125\n",
            "Frobenius norm: 2523.65478515625\n",
            "Frobenius norm: 2470.841796875\n",
            "Frobenius norm: 2489.457275390625\n",
            "Frobenius norm: 2511.62109375\n",
            "Frobenius norm: 2488.15185546875\n",
            "Frobenius norm: 2507.656982421875\n",
            "Frobenius norm: 2487.383056640625\n",
            "Frobenius norm: 2499.248291015625\n",
            "Frobenius norm: 2495.309814453125\n",
            "Frobenius norm: 2516.637939453125\n",
            "Frobenius norm: 2495.803466796875\n",
            "Frobenius norm: 2488.906982421875\n",
            "Frobenius norm: 2504.9853515625\n",
            "Frobenius norm: 2515.298828125\n",
            "Frobenius norm: 2503.307861328125\n",
            "Frobenius norm: 2537.056640625\n",
            "Frobenius norm: 2529.587890625\n",
            "Frobenius norm: 2493.790283203125\n",
            "Frobenius norm: 2496.3505859375\n",
            "Frobenius norm: 2516.26220703125\n",
            "Frobenius norm: 2514.110595703125\n",
            "Frobenius norm: 2478.25390625\n",
            "Frobenius norm: 2514.101318359375\n",
            "Frobenius norm: 2502.6982421875\n",
            "Frobenius norm: 2499.6337890625\n",
            "Frobenius norm: 2489.093017578125\n",
            "Frobenius norm: 2508.8955078125\n",
            "Frobenius norm: 2503.6064453125\n",
            "Frobenius norm: 2491.41796875\n",
            "Frobenius norm: 2549.695556640625\n",
            "Frobenius norm: 2491.204345703125\n",
            "Frobenius norm: 2527.239501953125\n",
            "Frobenius norm: 2503.132568359375\n",
            "Frobenius norm: 2503.869384765625\n",
            "Frobenius norm: 2530.23681640625\n",
            "Frobenius norm: 2492.84033203125\n",
            "Frobenius norm: 2458.481689453125\n",
            "Frobenius norm: 2517.7177734375\n",
            "Frobenius norm: 2460.64794921875\n",
            "Frobenius norm: 2509.91357421875\n",
            "Frobenius norm: 2503.04296875\n",
            "Frobenius norm: 2524.8935546875\n",
            "Frobenius norm: 2473.10986328125\n",
            "Frobenius norm: 2520.763671875\n",
            "Frobenius norm: 2500.520263671875\n",
            "Frobenius norm: 2493.7890625\n",
            "Frobenius norm: 2516.969970703125\n",
            "Frobenius norm: 2567.046142578125\n",
            "Frobenius norm: 2497.64794921875\n",
            "Frobenius norm: 2501.752685546875\n",
            "Frobenius norm: 2521.398193359375\n",
            "Frobenius norm: 2535.711669921875\n",
            "Frobenius norm: 2515.1767578125\n",
            "Frobenius norm: 2541.982177734375\n",
            "Frobenius norm: 2551.450927734375\n",
            "Frobenius norm: 2507.005615234375\n",
            "Frobenius norm: 2516.05859375\n",
            "Frobenius norm: 2512.150146484375\n",
            "Frobenius norm: 2546.552978515625\n",
            "Frobenius norm: 2526.461669921875\n",
            "Frobenius norm: 2501.006103515625\n",
            "Frobenius norm: 2518.0185546875\n",
            "Frobenius norm: 2523.197021484375\n",
            "Frobenius norm: 2530.787109375\n",
            "Frobenius norm: 2486.28173828125\n",
            "Frobenius norm: 2515.149169921875\n",
            "Frobenius norm: 2532.21923828125\n",
            "Frobenius norm: 2561.944091796875\n",
            "Frobenius norm: 2483.127197265625\n",
            "Frobenius norm: 2512.00048828125\n",
            "Frobenius norm: 2522.734130859375\n",
            "Frobenius norm: 2519.49609375\n",
            "Frobenius norm: 2547.525146484375\n",
            "Frobenius norm: 2516.003662109375\n",
            "Frobenius norm: 2552.150634765625\n",
            "Frobenius norm: 2499.078857421875\n",
            "Frobenius norm: 2508.80859375\n",
            "Frobenius norm: 2508.375732421875\n",
            "Frobenius norm: 2524.241943359375\n",
            "Frobenius norm: 2468.157470703125\n",
            "Frobenius norm: 2528.610107421875\n",
            "Frobenius norm: 2534.789794921875\n",
            "Frobenius norm: 2532.177490234375\n",
            "Frobenius norm: 2504.920654296875\n",
            "Frobenius norm: 2482.024658203125\n",
            "Frobenius norm: 2521.54150390625\n",
            "Frobenius norm: 2497.526123046875\n",
            "Frobenius norm: 2509.1435546875\n",
            "Frobenius norm: 2530.033935546875\n",
            "Frobenius norm: 2472.99169921875\n",
            "Frobenius norm: 2517.919677734375\n",
            "Frobenius norm: 2471.2880859375\n",
            "Frobenius norm: 2494.271484375\n",
            "Frobenius norm: 2491.0400390625\n",
            "Frobenius norm: 2513.6240234375\n",
            "Frobenius norm: 2488.181884765625\n",
            "Frobenius norm: 2487.51416015625\n",
            "Frobenius norm: 2530.568115234375\n",
            "Frobenius norm: 2523.468994140625\n",
            "Frobenius norm: 2515.531005859375\n",
            "Frobenius norm: 2505.970458984375\n",
            "Frobenius norm: 2504.53955078125\n",
            "Frobenius norm: 2485.3515625\n",
            "Frobenius norm: 2501.6484375\n",
            "Frobenius norm: 2529.63525390625\n",
            "Frobenius norm: 2510.277587890625\n",
            "Frobenius norm: 2537.70166015625\n",
            "Frobenius norm: 2473.013427734375\n",
            "Frobenius norm: 2526.195068359375\n",
            "Frobenius norm: 2496.150146484375\n",
            "Frobenius norm: 2516.133056640625\n",
            "Frobenius norm: 2523.567626953125\n",
            "Frobenius norm: 2533.543212890625\n",
            "Frobenius norm: 2526.04345703125\n",
            "Frobenius norm: 2533.504150390625\n",
            "Frobenius norm: 2527.50732421875\n",
            "Frobenius norm: 2506.7705078125\n",
            "Frobenius norm: 2488.98388671875\n",
            "Frobenius norm: 2489.33544921875\n",
            "Frobenius norm: 2514.552734375\n",
            "Frobenius norm: 2491.37451171875\n",
            "Frobenius norm: 2503.633544921875\n",
            "Frobenius norm: 2482.8955078125\n",
            "Frobenius norm: 2508.360107421875\n",
            "Frobenius norm: 2517.211669921875\n",
            "Frobenius norm: 2513.056396484375\n",
            "Frobenius norm: 2522.4853515625\n",
            "Frobenius norm: 2483.795654296875\n",
            "Frobenius norm: 2497.515869140625\n",
            "Frobenius norm: 2551.67333984375\n",
            "Frobenius norm: 2497.185546875\n",
            "Frobenius norm: 2509.37109375\n",
            "Frobenius norm: 2500.36279296875\n",
            "Frobenius norm: 2467.369140625\n",
            "Frobenius norm: 2476.537109375\n",
            "Frobenius norm: 2541.365234375\n",
            "Frobenius norm: 2496.56640625\n",
            "Frobenius norm: 2494.664794921875\n",
            "Frobenius norm: 2507.58447265625\n",
            "Frobenius norm: 2511.90283203125\n",
            "Frobenius norm: 2529.8427734375\n",
            "Frobenius norm: 2505.611572265625\n",
            "Frobenius norm: 2513.77783203125\n",
            "Frobenius norm: 2491.78857421875\n",
            "Frobenius norm: 2499.43798828125\n",
            "Frobenius norm: 2527.0224609375\n",
            "Frobenius norm: 2509.236328125\n",
            "Frobenius norm: 2527.399169921875\n",
            "Frobenius norm: 2507.58837890625\n",
            "Frobenius norm: 2496.318603515625\n",
            "Frobenius norm: 2502.79296875\n",
            "Frobenius norm: 2514.90087890625\n",
            "Frobenius norm: 2561.9228515625\n",
            "Frobenius norm: 2501.84521484375\n",
            "Frobenius norm: 2510.12841796875\n",
            "Frobenius norm: 2517.8701171875\n",
            "Frobenius norm: 2507.218505859375\n",
            "Frobenius norm: 2535.203125\n",
            "Frobenius norm: 2514.9267578125\n",
            "Frobenius norm: 2522.347412109375\n",
            "Frobenius norm: 2523.5908203125\n",
            "Frobenius norm: 2506.76123046875\n",
            "Frobenius norm: 2523.04296875\n",
            "Frobenius norm: 2534.66748046875\n",
            "Frobenius norm: 2502.353271484375\n",
            "Frobenius norm: 2510.282470703125\n",
            "Frobenius norm: 2481.647705078125\n",
            "Frobenius norm: 2498.334228515625\n",
            "Frobenius norm: 2496.471923828125\n",
            "Frobenius norm: 2512.993896484375\n",
            "Frobenius norm: 2487.84521484375\n",
            "Frobenius norm: 2522.15185546875\n",
            "Frobenius norm: 2477.787353515625\n",
            "Frobenius norm: 2475.745361328125\n",
            "Frobenius norm: 2503.6923828125\n",
            "Frobenius norm: 2469.404052734375\n",
            "Frobenius norm: 2523.499267578125\n",
            "Frobenius norm: 2500.84619140625\n",
            "Frobenius norm: 2449.17431640625\n",
            "Frobenius norm: 2504.896484375\n",
            "Frobenius norm: 2527.311767578125\n",
            "Frobenius norm: 2548.143310546875\n",
            "Frobenius norm: 2453.146484375\n",
            "Frobenius norm: 2516.43994140625\n",
            "Frobenius norm: 2525.61083984375\n",
            "Frobenius norm: 2477.206298828125\n",
            "Frobenius norm: 2521.11572265625\n",
            "Frobenius norm: 2551.724853515625\n",
            "Frobenius norm: 2510.534423828125\n",
            "Frobenius norm: 2481.8779296875\n",
            "Frobenius norm: 2500.850341796875\n",
            "Frobenius norm: 2502.41064453125\n",
            "Frobenius norm: 2522.284912109375\n",
            "Frobenius norm: 2486.56494140625\n",
            "Frobenius norm: 2502.5830078125\n",
            "Frobenius norm: 2532.26123046875\n",
            "Frobenius norm: 2471.098388671875\n",
            "Frobenius norm: 2500.63232421875\n",
            "Frobenius norm: 2537.743896484375\n",
            "Frobenius norm: 2504.160400390625\n",
            "Frobenius norm: 2555.853759765625\n",
            "Frobenius norm: 2506.326416015625\n",
            "Frobenius norm: 2487.87841796875\n",
            "Frobenius norm: 2514.884521484375\n",
            "Frobenius norm: 2502.988525390625\n",
            "Frobenius norm: 2488.90380859375\n",
            "Frobenius norm: 2497.69091796875\n",
            "Frobenius norm: 2491.932861328125\n",
            "Frobenius norm: 2501.44580078125\n",
            "Frobenius norm: 2512.513916015625\n",
            "Frobenius norm: 2519.2783203125\n",
            "Frobenius norm: 2527.822265625\n",
            "Frobenius norm: 2525.255859375\n",
            "Frobenius norm: 2489.3203125\n",
            "Frobenius norm: 2541.364501953125\n",
            "Frobenius norm: 2534.417724609375\n",
            "Frobenius norm: 2485.879638671875\n",
            "Frobenius norm: 2536.603271484375\n",
            "Frobenius norm: 2508.758544921875\n",
            "Frobenius norm: 2506.911865234375\n",
            "Frobenius norm: 2489.30517578125\n",
            "Frobenius norm: 2542.937255859375\n",
            "Frobenius norm: 2534.529541015625\n",
            "Frobenius norm: 2508.504638671875\n",
            "Frobenius norm: 2509.203369140625\n",
            "Frobenius norm: 2496.447509765625\n",
            "Frobenius norm: 2545.898193359375\n",
            "Frobenius norm: 2491.097412109375\n",
            "Frobenius norm: 2507.5\n",
            "Frobenius norm: 2507.605712890625\n",
            "Frobenius norm: 2498.455322265625\n",
            "Frobenius norm: 2515.1943359375\n",
            "Frobenius norm: 2516.7783203125\n",
            "Frobenius norm: 2500.94140625\n",
            "Frobenius norm: 2536.44677734375\n",
            "Frobenius norm: 2516.528564453125\n",
            "Frobenius norm: 2462.48876953125\n",
            "Frobenius norm: 2481.62255859375\n",
            "Frobenius norm: 2513.224365234375\n",
            "Frobenius norm: 2502.195068359375\n",
            "Frobenius norm: 2501.45947265625\n",
            "Frobenius norm: 2515.009765625\n",
            "Frobenius norm: 2498.595703125\n",
            "Frobenius norm: 2488.489013671875\n",
            "Frobenius norm: 2494.583251953125\n",
            "Frobenius norm: 2547.231201171875\n",
            "Frobenius norm: 2483.58349609375\n",
            "Frobenius norm: 2554.005126953125\n",
            "Frobenius norm: 2519.572265625\n",
            "Frobenius norm: 2499.113037109375\n",
            "Frobenius norm: 2505.125244140625\n",
            "Frobenius norm: 2506.8896484375\n",
            "Frobenius norm: 2496.483642578125\n",
            "Frobenius norm: 2503.95263671875\n",
            "Frobenius norm: 2515.92919921875\n",
            "Frobenius norm: 2504.906005859375\n",
            "Frobenius norm: 2521.426513671875\n",
            "Frobenius norm: 2519.791259765625\n",
            "Frobenius norm: 2512.787109375\n",
            "Frobenius norm: 2489.859619140625\n",
            "Frobenius norm: 2486.9189453125\n",
            "Frobenius norm: 2497.62939453125\n",
            "Frobenius norm: 2494.21728515625\n",
            "Frobenius norm: 2489.6416015625\n",
            "Frobenius norm: 2461.3310546875\n",
            "Frobenius norm: 2481.979736328125\n",
            "Frobenius norm: 2542.149169921875\n",
            "Frobenius norm: 2512.22216796875\n",
            "Frobenius norm: 2525.644287109375\n",
            "Frobenius norm: 2497.145751953125\n",
            "Frobenius norm: 2479.196044921875\n",
            "Frobenius norm: 2509.75244140625\n",
            "Frobenius norm: 2498.02099609375\n",
            "Frobenius norm: 2495.11376953125\n",
            "Frobenius norm: 2525.353271484375\n",
            "Frobenius norm: 2540.09619140625\n",
            "Frobenius norm: 2525.003173828125\n",
            "Frobenius norm: 2539.040771484375\n",
            "Frobenius norm: 2528.373291015625\n",
            "Frobenius norm: 2536.021240234375\n",
            "Frobenius norm: 2496.867919921875\n",
            "Frobenius norm: 2547.221923828125\n",
            "Frobenius norm: 2517.384521484375\n",
            "Frobenius norm: 2500.78369140625\n",
            "Frobenius norm: 2485.174560546875\n",
            "Frobenius norm: 2536.96240234375\n",
            "Frobenius norm: 2496.0732421875\n",
            "Frobenius norm: 2480.735107421875\n",
            "Frobenius norm: 2525.67919921875\n",
            "Frobenius norm: 2500.3984375\n",
            "Frobenius norm: 2527.718505859375\n",
            "Frobenius norm: 2477.664306640625\n",
            "Frobenius norm: 2526.6083984375\n",
            "Frobenius norm: 2521.483154296875\n",
            "Frobenius norm: 2505.100341796875\n",
            "Frobenius norm: 2519.89794921875\n",
            "Frobenius norm: 2499.630615234375\n",
            "Frobenius norm: 2551.03173828125\n",
            "Frobenius norm: 2514.60107421875\n",
            "Frobenius norm: 2528.29052734375\n",
            "Frobenius norm: 2516.78466796875\n",
            "Frobenius norm: 2512.54150390625\n",
            "Frobenius norm: 2497.707763671875\n",
            "Frobenius norm: 2540.51953125\n",
            "Frobenius norm: 2495.138427734375\n",
            "Frobenius norm: 2505.05810546875\n",
            "Frobenius norm: 2533.191162109375\n",
            "Frobenius norm: 2484.285888671875\n",
            "Frobenius norm: 2506.521484375\n",
            "Frobenius norm: 2531.3564453125\n",
            "Frobenius norm: 2518.59716796875\n",
            "Frobenius norm: 2512.663818359375\n",
            "Frobenius norm: 2482.7138671875\n",
            "Frobenius norm: 2543.653564453125\n",
            "Frobenius norm: 2491.299072265625\n",
            "Frobenius norm: 2493.507080078125\n",
            "Frobenius norm: 2508.079833984375\n",
            "Frobenius norm: 2537.35205078125\n",
            "Frobenius norm: 2492.4619140625\n",
            "Frobenius norm: 2475.67578125\n",
            "Frobenius norm: 2521.972900390625\n",
            "Frobenius norm: 2525.016845703125\n",
            "Frobenius norm: 2488.281982421875\n",
            "Frobenius norm: 2507.45556640625\n",
            "Frobenius norm: 2493.05078125\n",
            "Frobenius norm: 2485.405517578125\n",
            "Frobenius norm: 2498.444091796875\n",
            "Frobenius norm: 2498.598876953125\n",
            "Frobenius norm: 2507.588134765625\n",
            "Frobenius norm: 2525.471923828125\n",
            "Frobenius norm: 2522.59619140625\n",
            "Frobenius norm: 2533.937255859375\n",
            "Frobenius norm: 2501.671142578125\n",
            "Frobenius norm: 2492.200927734375\n",
            "Frobenius norm: 2540.478759765625\n",
            "Frobenius norm: 2535.926025390625\n",
            "Frobenius norm: 2536.27587890625\n",
            "Frobenius norm: 2530.916015625\n",
            "Frobenius norm: 2518.953369140625\n",
            "Frobenius norm: 2530.060791015625\n",
            "Frobenius norm: 2514.568115234375\n",
            "Frobenius norm: 2496.5595703125\n",
            "Frobenius norm: 2500.693359375\n",
            "Frobenius norm: 2496.079345703125\n",
            "Frobenius norm: 2510.717041015625\n",
            "Frobenius norm: 2499.584716796875\n",
            "Frobenius norm: 2515.867919921875\n",
            "Frobenius norm: 2487.63330078125\n",
            "Frobenius norm: 2515.95703125\n",
            "Frobenius norm: 2525.792724609375\n",
            "Frobenius norm: 2531.39599609375\n",
            "Frobenius norm: 2493.18017578125\n",
            "Frobenius norm: 2507.138427734375\n",
            "Frobenius norm: 2548.740478515625\n",
            "Frobenius norm: 2474.218994140625\n",
            "Frobenius norm: 2510.642822265625\n",
            "Frobenius norm: 2491.003173828125\n",
            "Frobenius norm: 2501.60791015625\n",
            "Frobenius norm: 2511.459716796875\n",
            "Frobenius norm: 2493.521484375\n",
            "Frobenius norm: 2471.669921875\n",
            "Frobenius norm: 2512.2568359375\n",
            "Frobenius norm: 2511.66845703125\n",
            "Frobenius norm: 2496.529541015625\n",
            "Frobenius norm: 2525.076171875\n",
            "Frobenius norm: 2472.274658203125\n",
            "Frobenius norm: 2513.968994140625\n",
            "Frobenius norm: 2506.16845703125\n",
            "Frobenius norm: 2475.9140625\n",
            "Frobenius norm: 2518.205322265625\n",
            "Frobenius norm: 2497.4501953125\n",
            "Frobenius norm: 2520.70849609375\n",
            "Frobenius norm: 2507.660400390625\n",
            "Frobenius norm: 2515.32177734375\n",
            "Frobenius norm: 2483.21923828125\n",
            "Frobenius norm: 2519.626953125\n",
            "Frobenius norm: 2487.842041015625\n",
            "Frobenius norm: 2501.71435546875\n",
            "Frobenius norm: 2524.318115234375\n",
            "Frobenius norm: 2459.0400390625\n",
            "Frobenius norm: 2498.88818359375\n",
            "Frobenius norm: 2482.95166015625\n",
            "Frobenius norm: 2507.035888671875\n",
            "Frobenius norm: 2528.5224609375\n",
            "Frobenius norm: 2475.37548828125\n",
            "Frobenius norm: 2506.744873046875\n",
            "Frobenius norm: 2495.210205078125\n",
            "Frobenius norm: 2520.521484375\n",
            "Frobenius norm: 2529.501953125\n",
            "Frobenius norm: 2504.51220703125\n",
            "Frobenius norm: 2525.213623046875\n",
            "Frobenius norm: 2512.830078125\n",
            "Frobenius norm: 2524.6171875\n",
            "Frobenius norm: 2499.16796875\n",
            "Frobenius norm: 2557.485595703125\n",
            "Frobenius norm: 2491.53955078125\n",
            "Frobenius norm: 2449.51513671875\n",
            "Frobenius norm: 2506.03076171875\n",
            "Frobenius norm: 2512.07861328125\n",
            "Frobenius norm: 2499.007080078125\n",
            "Frobenius norm: 2525.930908203125\n",
            "Frobenius norm: 2510.8916015625\n",
            "Frobenius norm: 2486.72705078125\n",
            "Frobenius norm: 2481.4296875\n",
            "Frobenius norm: 2554.769287109375\n",
            "Frobenius norm: 2524.18017578125\n",
            "Frobenius norm: 2484.66015625\n",
            "Frobenius norm: 2442.40771484375\n",
            "Frobenius norm: 2485.8779296875\n",
            "Frobenius norm: 2486.771484375\n",
            "Frobenius norm: 2536.0546875\n",
            "Frobenius norm: 2519.865966796875\n",
            "Frobenius norm: 2504.911376953125\n",
            "Frobenius norm: 2508.521240234375\n",
            "Frobenius norm: 2481.255859375\n",
            "Frobenius norm: 2532.1259765625\n",
            "Frobenius norm: 2504.775146484375\n",
            "Frobenius norm: 2478.325927734375\n",
            "Frobenius norm: 2502.12451171875\n",
            "Frobenius norm: 2492.307861328125\n",
            "Frobenius norm: 2518.0087890625\n",
            "Frobenius norm: 2516.2275390625\n",
            "Frobenius norm: 2542.3798828125\n",
            "Frobenius norm: 2513.547607421875\n",
            "Frobenius norm: 2534.6201171875\n",
            "Frobenius norm: 2494.17333984375\n",
            "Frobenius norm: 2541.45263671875\n",
            "Frobenius norm: 2522.27734375\n",
            "Frobenius norm: 2501.19775390625\n",
            "Frobenius norm: 2525.029296875\n",
            "Frobenius norm: 2459.20166015625\n",
            "Frobenius norm: 2505.979736328125\n",
            "Frobenius norm: 2515.525634765625\n",
            "Frobenius norm: 2489.910888671875\n",
            "Frobenius norm: 2518.341796875\n",
            "Frobenius norm: 2517.885986328125\n",
            "Frobenius norm: 2511.249755859375\n",
            "Frobenius norm: 2498.261474609375\n",
            "Frobenius norm: 2499.15625\n",
            "Frobenius norm: 2533.7470703125\n",
            "Frobenius norm: 2507.624755859375\n",
            "Frobenius norm: 2504.5947265625\n",
            "Frobenius norm: 2560.868408203125\n",
            "Frobenius norm: 2484.835693359375\n",
            "Frobenius norm: 2471.13623046875\n",
            "Frobenius norm: 2518.861328125\n",
            "Frobenius norm: 2500.837158203125\n",
            "Frobenius norm: 2499.788330078125\n",
            "Frobenius norm: 2500.188232421875\n",
            "Frobenius norm: 2532.022216796875\n",
            "Frobenius norm: 2484.25146484375\n",
            "Frobenius norm: 2501.351318359375\n",
            "Frobenius norm: 2497.67236328125\n",
            "Frobenius norm: 2512.005126953125\n",
            "Frobenius norm: 2480.821044921875\n",
            "Frobenius norm: 2506.09521484375\n",
            "Frobenius norm: 2470.530029296875\n",
            "Frobenius norm: 2494.481201171875\n",
            "Frobenius norm: 2516.22705078125\n",
            "Frobenius norm: 2537.143310546875\n",
            "Frobenius norm: 2491.94921875\n",
            "Frobenius norm: 2499.54736328125\n",
            "Frobenius norm: 2491.609619140625\n",
            "Frobenius norm: 2528.78955078125\n",
            "Frobenius norm: 2510.667724609375\n",
            "Frobenius norm: 2489.110595703125\n",
            "Frobenius norm: 2541.0634765625\n",
            "Frobenius norm: 2484.807373046875\n",
            "Frobenius norm: 2500.28173828125\n",
            "Frobenius norm: 2498.031494140625\n",
            "Frobenius norm: 2566.991455078125\n",
            "Frobenius norm: 2458.394775390625\n",
            "Frobenius norm: 2467.27392578125\n",
            "Frobenius norm: 2509.6748046875\n",
            "Frobenius norm: 2510.807373046875\n",
            "Frobenius norm: 2532.33203125\n",
            "Frobenius norm: 2555.779541015625\n",
            "Frobenius norm: 2545.39794921875\n",
            "Frobenius norm: 2512.7626953125\n",
            "Frobenius norm: 2547.81982421875\n",
            "Frobenius norm: 2508.686279296875\n",
            "Frobenius norm: 2519.536376953125\n",
            "Frobenius norm: 2507.195556640625\n",
            "Frobenius norm: 2500.912841796875\n",
            "Frobenius norm: 2528.39599609375\n",
            "Frobenius norm: 2506.44091796875\n",
            "Frobenius norm: 2519.55224609375\n",
            "Frobenius norm: 2517.383056640625\n",
            "Frobenius norm: 2490.908935546875\n",
            "Frobenius norm: 2499.532470703125\n",
            "Frobenius norm: 2484.599853515625\n",
            "Frobenius norm: 2488.276123046875\n",
            "Frobenius norm: 2511.009521484375\n",
            "Frobenius norm: 2519.254150390625\n",
            "Frobenius norm: 2488.897705078125\n",
            "Frobenius norm: 2555.251953125\n",
            "Frobenius norm: 2528.8564453125\n",
            "Frobenius norm: 2534.295654296875\n",
            "Frobenius norm: 2523.8701171875\n",
            "Frobenius norm: 2538.190185546875\n",
            "Frobenius norm: 2495.464599609375\n",
            "Frobenius norm: 2535.4462890625\n",
            "Frobenius norm: 2488.4677734375\n",
            "Frobenius norm: 2528.39208984375\n",
            "Frobenius norm: 2476.526123046875\n",
            "Frobenius norm: 2521.849853515625\n",
            "Frobenius norm: 2481.906982421875\n",
            "Frobenius norm: 2516.048095703125\n",
            "Frobenius norm: 2540.848876953125\n",
            "Frobenius norm: 2468.40869140625\n",
            "Frobenius norm: 2535.850830078125\n",
            "Frobenius norm: 2496.1328125\n",
            "Frobenius norm: 2535.165771484375\n",
            "Frobenius norm: 2509.754150390625\n",
            "Frobenius norm: 2493.002197265625\n",
            "Frobenius norm: 2500.006103515625\n",
            "Frobenius norm: 2494.78662109375\n",
            "Frobenius norm: 2481.172607421875\n",
            "Frobenius norm: 2547.558837890625\n",
            "Frobenius norm: 2482.15283203125\n",
            "Frobenius norm: 2491.966796875\n",
            "Frobenius norm: 2540.422119140625\n",
            "Frobenius norm: 2529.037353515625\n",
            "Frobenius norm: 2474.70947265625\n",
            "Frobenius norm: 2505.68603515625\n",
            "Frobenius norm: 2528.17724609375\n",
            "Frobenius norm: 2508.346435546875\n",
            "Frobenius norm: 2560.151611328125\n",
            "Frobenius norm: 2518.04541015625\n",
            "Frobenius norm: 2494.306640625\n",
            "Frobenius norm: 2528.676025390625\n",
            "Frobenius norm: 2513.5439453125\n",
            "Frobenius norm: 2513.587158203125\n",
            "Frobenius norm: 2520.8544921875\n",
            "Frobenius norm: 2510.1552734375\n",
            "Frobenius norm: 2482.73193359375\n",
            "Frobenius norm: 2503.468505859375\n",
            "Frobenius norm: 2520.689697265625\n",
            "Frobenius norm: 2547.689697265625\n",
            "Frobenius norm: 2500.6220703125\n",
            "Frobenius norm: 2489.332763671875\n",
            "Frobenius norm: 2518.486572265625\n",
            "Frobenius norm: 2528.402099609375\n",
            "Frobenius norm: 2457.843505859375\n",
            "Frobenius norm: 2523.6259765625\n",
            "Frobenius norm: 2478.383056640625\n",
            "Frobenius norm: 2482.349609375\n",
            "Frobenius norm: 2480.2919921875\n",
            "Frobenius norm: 2504.829833984375\n",
            "Frobenius norm: 2504.5185546875\n",
            "Frobenius norm: 2513.311767578125\n",
            "Frobenius norm: 2508.655517578125\n",
            "Frobenius norm: 2477.261962890625\n",
            "Frobenius norm: 2507.832763671875\n",
            "Frobenius norm: 2509.09619140625\n",
            "Frobenius norm: 2524.5556640625\n",
            "Frobenius norm: 2490.60009765625\n",
            "Frobenius norm: 2527.789306640625\n",
            "Frobenius norm: 2505.546142578125\n",
            "Frobenius norm: 2532.7841796875\n",
            "Frobenius norm: 2477.701416015625\n",
            "Frobenius norm: 2523.57763671875\n",
            "Frobenius norm: 2506.4658203125\n",
            "Frobenius norm: 2482.881103515625\n",
            "Frobenius norm: 2514.181396484375\n",
            "Frobenius norm: 2499.701904296875\n",
            "Frobenius norm: 2495.752685546875\n",
            "Frobenius norm: 2492.736572265625\n",
            "Frobenius norm: 2504.185302734375\n",
            "Frobenius norm: 2502.70751953125\n",
            "Frobenius norm: 2484.65380859375\n",
            "Frobenius norm: 2513.478759765625\n",
            "Frobenius norm: 2539.883544921875\n",
            "Frobenius norm: 2496.937744140625\n",
            "Frobenius norm: 2531.544677734375\n",
            "Frobenius norm: 2496.752197265625\n",
            "Frobenius norm: 2499.788330078125\n",
            "Frobenius norm: 2508.0927734375\n",
            "Frobenius norm: 2510.078369140625\n",
            "Frobenius norm: 2515.253662109375\n",
            "Frobenius norm: 2494.457275390625\n",
            "Frobenius norm: 2488.26318359375\n",
            "Frobenius norm: 2499.5390625\n",
            "Frobenius norm: 2498.130126953125\n",
            "Frobenius norm: 2510.24462890625\n",
            "Frobenius norm: 2527.59033203125\n",
            "Frobenius norm: 2512.00439453125\n",
            "Frobenius norm: 2503.134521484375\n",
            "Frobenius norm: 2521.957275390625\n",
            "Frobenius norm: 2533.119140625\n",
            "Frobenius norm: 2493.210693359375\n",
            "Frobenius norm: 2488.75537109375\n",
            "Frobenius norm: 2516.2138671875\n",
            "Frobenius norm: 2517.5625\n",
            "Frobenius norm: 2513.387451171875\n",
            "Frobenius norm: 2506.66064453125\n",
            "Frobenius norm: 2529.566650390625\n",
            "Frobenius norm: 2543.400390625\n",
            "Frobenius norm: 2501.406005859375\n",
            "Frobenius norm: 2493.56591796875\n",
            "Frobenius norm: 2546.754638671875\n",
            "Frobenius norm: 2493.827392578125\n",
            "Frobenius norm: 2522.539306640625\n",
            "Frobenius norm: 2490.378173828125\n",
            "Frobenius norm: 2499.8876953125\n",
            "Frobenius norm: 2526.7470703125\n",
            "Frobenius norm: 2536.36474609375\n",
            "Frobenius norm: 2519.76611328125\n",
            "Frobenius norm: 2495.94775390625\n",
            "Frobenius norm: 2527.85205078125\n",
            "Frobenius norm: 2515.606201171875\n",
            "Frobenius norm: 2519.3759765625\n",
            "Frobenius norm: 2523.59716796875\n",
            "Frobenius norm: 2527.3818359375\n",
            "Frobenius norm: 2528.67626953125\n",
            "Frobenius norm: 2505.153564453125\n",
            "Frobenius norm: 2511.105224609375\n",
            "Frobenius norm: 2518.607177734375\n",
            "Frobenius norm: 2526.2412109375\n",
            "Frobenius norm: 2513.70703125\n",
            "Frobenius norm: 2530.718017578125\n",
            "Frobenius norm: 2501.44970703125\n",
            "Frobenius norm: 2509.685546875\n",
            "Frobenius norm: 2506.749267578125\n",
            "Frobenius norm: 2498.214111328125\n",
            "Frobenius norm: 2520.41357421875\n",
            "Frobenius norm: 2538.69189453125\n",
            "Frobenius norm: 2501.31689453125\n",
            "Frobenius norm: 2507.695556640625\n",
            "Frobenius norm: 2474.210693359375\n",
            "Frobenius norm: 2520.848388671875\n",
            "Frobenius norm: 2522.368896484375\n",
            "Frobenius norm: 2506.1279296875\n",
            "Frobenius norm: 2480.954345703125\n",
            "Frobenius norm: 2540.777099609375\n",
            "Frobenius norm: 2491.60400390625\n",
            "Frobenius norm: 2479.638916015625\n",
            "Frobenius norm: 2533.965576171875\n",
            "Frobenius norm: 2493.473876953125\n",
            "Frobenius norm: 2516.060302734375\n",
            "Frobenius norm: 2535.574462890625\n",
            "Frobenius norm: 2514.7900390625\n",
            "Frobenius norm: 2514.78271484375\n",
            "Frobenius norm: 2474.793212890625\n",
            "Frobenius norm: 2506.07275390625\n",
            "Frobenius norm: 2514.855712890625\n",
            "Frobenius norm: 2489.591064453125\n",
            "Frobenius norm: 2514.523193359375\n",
            "Frobenius norm: 2503.654541015625\n",
            "Frobenius norm: 2503.96337890625\n",
            "Frobenius norm: 2480.158935546875\n",
            "Frobenius norm: 2506.751220703125\n",
            "Frobenius norm: 2507.51953125\n",
            "Frobenius norm: 2494.244384765625\n",
            "Frobenius norm: 2525.6396484375\n",
            "Frobenius norm: 2496.643798828125\n",
            "Frobenius norm: 2547.603759765625\n",
            "Frobenius norm: 2520.48291015625\n",
            "Frobenius norm: 2531.403564453125\n",
            "Frobenius norm: 2540.9765625\n",
            "Frobenius norm: 2506.77392578125\n",
            "Frobenius norm: 2479.304931640625\n",
            "Frobenius norm: 2484.266845703125\n",
            "Frobenius norm: 2513.118896484375\n",
            "Frobenius norm: 2498.9775390625\n",
            "Frobenius norm: 2512.714599609375\n",
            "Frobenius norm: 2497.875244140625\n",
            "Frobenius norm: 2504.769775390625\n",
            "Frobenius norm: 2548.12353515625\n",
            "Frobenius norm: 2541.576171875\n",
            "Frobenius norm: 2518.37646484375\n",
            "Frobenius norm: 2529.51318359375\n",
            "Frobenius norm: 2489.6689453125\n",
            "Frobenius norm: 2444.07861328125\n",
            "Frobenius norm: 2520.382080078125\n",
            "Frobenius norm: 2527.858154296875\n",
            "Frobenius norm: 2550.958740234375\n",
            "Frobenius norm: 2496.734375\n",
            "Frobenius norm: 2503.305908203125\n",
            "Frobenius norm: 2513.51953125\n",
            "Frobenius norm: 2506.112060546875\n",
            "Frobenius norm: 2508.2265625\n",
            "Frobenius norm: 2521.064208984375\n",
            "Frobenius norm: 2497.00390625\n",
            "Frobenius norm: 2493.300537109375\n",
            "Frobenius norm: 2491.45361328125\n",
            "Frobenius norm: 2522.577880859375\n",
            "Frobenius norm: 2476.10595703125\n",
            "Frobenius norm: 2480.86962890625\n",
            "Frobenius norm: 2511.264892578125\n",
            "Frobenius norm: 2489.613525390625\n",
            "Frobenius norm: 2494.172119140625\n",
            "Frobenius norm: 2493.82568359375\n",
            "Frobenius norm: 2514.381591796875\n",
            "Frobenius norm: 2491.202880859375\n",
            "Frobenius norm: 2482.31494140625\n",
            "Frobenius norm: 2528.387939453125\n",
            "Frobenius norm: 2519.33251953125\n",
            "Frobenius norm: 2510.444091796875\n",
            "Frobenius norm: 2479.67529296875\n",
            "Frobenius norm: 2504.025390625\n",
            "Frobenius norm: 2508.914794921875\n",
            "Frobenius norm: 2507.910400390625\n",
            "Frobenius norm: 2523.80517578125\n",
            "Frobenius norm: 2485.309814453125\n",
            "Frobenius norm: 2482.30810546875\n",
            "Frobenius norm: 2539.51123046875\n",
            "Frobenius norm: 2545.13232421875\n",
            "Frobenius norm: 2534.202392578125\n",
            "Frobenius norm: 2519.98388671875\n",
            "Frobenius norm: 2541.6220703125\n",
            "Frobenius norm: 2558.46533203125\n",
            "Frobenius norm: 2514.232177734375\n",
            "Frobenius norm: 2507.435546875\n",
            "Frobenius norm: 2513.82373046875\n",
            "Frobenius norm: 2483.410888671875\n",
            "Frobenius norm: 2507.992431640625\n",
            "Frobenius norm: 2502.90576171875\n",
            "Frobenius norm: 2489.04736328125\n",
            "Frobenius norm: 2516.856201171875\n",
            "Frobenius norm: 2522.17236328125\n",
            "Frobenius norm: 2514.699462890625\n",
            "Frobenius norm: 2499.394287109375\n",
            "Frobenius norm: 2526.248046875\n",
            "Frobenius norm: 2527.16015625\n",
            "Frobenius norm: 2514.3720703125\n",
            "Frobenius norm: 2558.025390625\n",
            "Frobenius norm: 2529.170166015625\n",
            "Frobenius norm: 2517.591064453125\n",
            "Frobenius norm: 2488.116943359375\n",
            "Frobenius norm: 2498.061279296875\n",
            "Frobenius norm: 2490.921630859375\n",
            "Frobenius norm: 2511.49755859375\n",
            "Frobenius norm: 2501.908203125\n",
            "Frobenius norm: 2481.275390625\n",
            "Frobenius norm: 2483.0693359375\n",
            "Frobenius norm: 2515.40869140625\n",
            "Frobenius norm: 2513.232177734375\n",
            "Frobenius norm: 2497.719482421875\n",
            "Frobenius norm: 2494.1396484375\n",
            "Frobenius norm: 2472.363525390625\n",
            "Frobenius norm: 2492.146240234375\n",
            "Frobenius norm: 2500.858154296875\n",
            "Frobenius norm: 2480.792724609375\n",
            "Frobenius norm: 2525.7890625\n",
            "Frobenius norm: 2503.440673828125\n",
            "Frobenius norm: 2495.8779296875\n",
            "Frobenius norm: 2534.14453125\n",
            "Frobenius norm: 2505.45654296875\n",
            "Frobenius norm: 2496.754638671875\n",
            "Frobenius norm: 2542.054443359375\n",
            "Frobenius norm: 2511.2001953125\n",
            "Frobenius norm: 2525.546630859375\n",
            "Frobenius norm: 2515.947021484375\n",
            "Frobenius norm: 2495.89306640625\n",
            "Frobenius norm: 2524.0634765625\n",
            "Frobenius norm: 2542.037841796875\n",
            "Frobenius norm: 2513.768798828125\n",
            "Frobenius norm: 2498.495849609375\n",
            "Frobenius norm: 2515.8662109375\n",
            "Frobenius norm: 2501.137939453125\n",
            "Frobenius norm: 2507.082763671875\n",
            "Frobenius norm: 2507.67578125\n",
            "Frobenius norm: 2530.2490234375\n",
            "Frobenius norm: 2521.45263671875\n",
            "Frobenius norm: 2539.39208984375\n",
            "Frobenius norm: 2493.407958984375\n",
            "Frobenius norm: 2515.517822265625\n",
            "Frobenius norm: 2540.02490234375\n",
            "Frobenius norm: 2536.234130859375\n",
            "Frobenius norm: 2475.967529296875\n",
            "Frobenius norm: 2511.921142578125\n",
            "Frobenius norm: 2522.93701171875\n",
            "Frobenius norm: 2544.9248046875\n",
            "Frobenius norm: 2526.82763671875\n",
            "Frobenius norm: 2491.968505859375\n",
            "Frobenius norm: 2501.25927734375\n",
            "Frobenius norm: 2546.885009765625\n",
            "Frobenius norm: 2500.352783203125\n",
            "Frobenius norm: 2499.8798828125\n",
            "Frobenius norm: 2554.412109375\n",
            "Frobenius norm: 2516.4091796875\n",
            "Frobenius norm: 2492.370361328125\n",
            "Frobenius norm: 2477.685302734375\n",
            "Frobenius norm: 2504.00048828125\n",
            "Frobenius norm: 2500.0712890625\n",
            "Frobenius norm: 2521.9501953125\n",
            "Frobenius norm: 2509.614990234375\n",
            "Frobenius norm: 2524.853271484375\n",
            "Frobenius norm: 2492.321044921875\n",
            "Frobenius norm: 2478.8212890625\n",
            "Frobenius norm: 2510.96923828125\n",
            "Frobenius norm: 2500.5537109375\n",
            "Frobenius norm: 2540.781494140625\n",
            "Frobenius norm: 2501.56494140625\n",
            "Frobenius norm: 2495.601806640625\n",
            "Frobenius norm: 2522.782470703125\n",
            "Frobenius norm: 2513.34228515625\n",
            "Frobenius norm: 2536.57861328125\n",
            "Frobenius norm: 2530.362060546875\n",
            "Frobenius norm: 2464.933837890625\n",
            "Frobenius norm: 2502.325927734375\n",
            "Frobenius norm: 2546.131103515625\n",
            "Frobenius norm: 2496.062744140625\n",
            "Frobenius norm: 2521.884765625\n",
            "Frobenius norm: 2509.8173828125\n",
            "Frobenius norm: 2504.6611328125\n",
            "Frobenius norm: 2550.006103515625\n",
            "Frobenius norm: 2508.197265625\n",
            "Frobenius norm: 2484.31787109375\n",
            "Frobenius norm: 2478.996337890625\n",
            "Frobenius norm: 2499.101318359375\n",
            "Frobenius norm: 2553.460693359375\n",
            "Frobenius norm: 2497.828369140625\n",
            "Frobenius norm: 2493.288330078125\n",
            "Frobenius norm: 2502.8837890625\n",
            "Frobenius norm: 2517.328857421875\n",
            "Frobenius norm: 2517.6259765625\n",
            "Frobenius norm: 2460.15087890625\n",
            "Frobenius norm: 2509.220458984375\n",
            "Frobenius norm: 2473.374267578125\n",
            "Frobenius norm: 2522.49853515625\n",
            "Frobenius norm: 2491.6533203125\n",
            "Frobenius norm: 2493.955810546875\n",
            "Frobenius norm: 2528.745849609375\n",
            "Frobenius norm: 2519.513427734375\n",
            "Frobenius norm: 2496.493408203125\n",
            "Frobenius norm: 2510.174072265625\n",
            "Frobenius norm: 2502.196533203125\n",
            "Frobenius norm: 2494.344482421875\n",
            "Frobenius norm: 2519.673583984375\n",
            "Frobenius norm: 2494.30908203125\n",
            "Frobenius norm: 2518.974853515625\n",
            "Frobenius norm: 2512.828857421875\n",
            "Frobenius norm: 2464.10986328125\n",
            "Frobenius norm: 2499.965087890625\n",
            "Frobenius norm: 2500.415771484375\n",
            "Frobenius norm: 2540.494384765625\n",
            "Frobenius norm: 2492.913818359375\n",
            "Frobenius norm: 2533.76025390625\n",
            "Frobenius norm: 2514.450439453125\n",
            "Frobenius norm: 2490.36572265625\n",
            "Frobenius norm: 2504.572998046875\n",
            "Frobenius norm: 2497.46728515625\n",
            "Frobenius norm: 2506.4658203125\n",
            "Frobenius norm: 2518.1416015625\n",
            "Frobenius norm: 2487.966064453125\n",
            "Frobenius norm: 2524.3701171875\n",
            "Frobenius norm: 2523.39453125\n",
            "Frobenius norm: 2495.67626953125\n",
            "Frobenius norm: 2523.47802734375\n",
            "Frobenius norm: 2547.42138671875\n",
            "Frobenius norm: 2516.8330078125\n",
            "Frobenius norm: 2510.38427734375\n",
            "Frobenius norm: 2520.982666015625\n",
            "Frobenius norm: 2502.6083984375\n",
            "Frobenius norm: 2517.396240234375\n",
            "Frobenius norm: 2530.9404296875\n",
            "Frobenius norm: 2526.813232421875\n",
            "Frobenius norm: 2513.29296875\n",
            "Frobenius norm: 2532.95166015625\n",
            "Frobenius norm: 2494.645751953125\n",
            "Frobenius norm: 2494.9482421875\n",
            "Frobenius norm: 2528.695556640625\n",
            "Frobenius norm: 2501.227783203125\n",
            "Frobenius norm: 2512.50048828125\n",
            "Frobenius norm: 2515.716796875\n",
            "Frobenius norm: 2501.8818359375\n",
            "Frobenius norm: 2511.601318359375\n",
            "Frobenius norm: 2536.2099609375\n",
            "Frobenius norm: 2508.06298828125\n",
            "Frobenius norm: 2495.340576171875\n",
            "Frobenius norm: 2533.27392578125\n",
            "Frobenius norm: 2506.66259765625\n",
            "Frobenius norm: 2516.59375\n",
            "Frobenius norm: 2488.57177734375\n",
            "Frobenius norm: 2501.782470703125\n",
            "Frobenius norm: 2528.304443359375\n",
            "Frobenius norm: 2519.883056640625\n",
            "Frobenius norm: 2523.746826171875\n",
            "Frobenius norm: 2518.699951171875\n",
            "Frobenius norm: 2500.3876953125\n",
            "Frobenius norm: 2502.478515625\n",
            "Frobenius norm: 2521.78955078125\n",
            "Frobenius norm: 2483.81005859375\n",
            "Frobenius norm: 2545.155517578125\n",
            "Frobenius norm: 2532.07421875\n",
            "Frobenius norm: 2500.481201171875\n",
            "Frobenius norm: 2475.439208984375\n",
            "Frobenius norm: 2524.5693359375\n",
            "Frobenius norm: 2529.69970703125\n",
            "Frobenius norm: 2484.029296875\n",
            "Frobenius norm: 2552.771240234375\n",
            "Frobenius norm: 2524.785400390625\n",
            "Frobenius norm: 2525.326171875\n",
            "Frobenius norm: 2514.169189453125\n",
            "Frobenius norm: 2485.6083984375\n",
            "Frobenius norm: 2466.733642578125\n",
            "Frobenius norm: 2513.216552734375\n",
            "Frobenius norm: 2475.40087890625\n",
            "Frobenius norm: 2500.592529296875\n",
            "Frobenius norm: 2522.091064453125\n",
            "Frobenius norm: 2538.86474609375\n",
            "Frobenius norm: 2535.74658203125\n",
            "Frobenius norm: 2504.07177734375\n",
            "Frobenius norm: 2515.635986328125\n",
            "Frobenius norm: 2519.84619140625\n",
            "Frobenius norm: 2517.2666015625\n",
            "Frobenius norm: 2503.85107421875\n",
            "Frobenius norm: 2517.864990234375\n",
            "Frobenius norm: 2510.01416015625\n",
            "Frobenius norm: 2532.13037109375\n",
            "Frobenius norm: 2509.209716796875\n",
            "Frobenius norm: 2490.996337890625\n",
            "Frobenius norm: 2477.631591796875\n",
            "Frobenius norm: 2532.17822265625\n",
            "Frobenius norm: 2547.5888671875\n",
            "Frobenius norm: 2507.304443359375\n",
            "Frobenius norm: 2472.408447265625\n",
            "Frobenius norm: 2501.385009765625\n",
            "Frobenius norm: 2513.583984375\n",
            "Frobenius norm: 2500.434814453125\n",
            "Frobenius norm: 2549.997314453125\n",
            "Frobenius norm: 2459.82763671875\n",
            "Frobenius norm: 2496.514892578125\n",
            "Frobenius norm: 2494.967529296875\n",
            "Frobenius norm: 2518.76318359375\n",
            "Frobenius norm: 2498.253173828125\n",
            "Frobenius norm: 2543.01806640625\n",
            "Frobenius norm: 2486.634521484375\n",
            "Frobenius norm: 2487.273681640625\n",
            "Frobenius norm: 2466.491943359375\n",
            "Frobenius norm: 2535.05712890625\n",
            "Frobenius norm: 2531.431396484375\n",
            "Frobenius norm: 2493.960693359375\n",
            "Frobenius norm: 2507.229736328125\n",
            "Frobenius norm: 2520.001220703125\n",
            "Frobenius norm: 2491.20068359375\n",
            "Frobenius norm: 2529.404296875\n",
            "Frobenius norm: 2518.967041015625\n",
            "Frobenius norm: 2546.82373046875\n",
            "Frobenius norm: 2532.510009765625\n",
            "Frobenius norm: 2491.85107421875\n",
            "Frobenius norm: 2548.92041015625\n",
            "Frobenius norm: 2525.271728515625\n",
            "Frobenius norm: 2480.355712890625\n",
            "Frobenius norm: 2488.51513671875\n",
            "Frobenius norm: 2492.802978515625\n",
            "Frobenius norm: 2492.9990234375\n",
            "Frobenius norm: 2509.25830078125\n",
            "Frobenius norm: 2514.370361328125\n",
            "Frobenius norm: 2545.68798828125\n",
            "Frobenius norm: 2539.8134765625\n",
            "Frobenius norm: 2500.353271484375\n",
            "Frobenius norm: 2515.0537109375\n",
            "Frobenius norm: 2500.90869140625\n",
            "Frobenius norm: 2502.304931640625\n",
            "Frobenius norm: 2496.858154296875\n",
            "Frobenius norm: 2507.302490234375\n",
            "Frobenius norm: 2524.98681640625\n",
            "Frobenius norm: 2506.46826171875\n",
            "Frobenius norm: 2499.01953125\n",
            "Frobenius norm: 2516.80859375\n",
            "Frobenius norm: 2505.165771484375\n",
            "Frobenius norm: 2506.666259765625\n",
            "Total time on mps is 0.43739891 seconds\n"
          ]
        }
      ],
      "source": [
        "torch.mps.synchronize()\n",
        "start = time.time()\n",
        "for _ in range(1000):\n",
        "    A = torch.rand(100,100,device=device)\n",
        "    B = torch.rand(100,100,device=device)\n",
        "    res = torch.mm(A,B)\n",
        "    frobeniusNorm = torch.norm(res,p='fro')\n",
        "    print(f\"Frobenius norm: {frobeniusNorm}\")\n",
        "torch.mps.synchronize()    \n",
        "end = time.time()\n",
        "time3 = end - start\n",
        "print(f'Total time on {device} is {time3:.8f} seconds')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "r is tensor([2524.5066, 2484.2039, 2498.2095, 2523.1694, 2515.9666, 2531.3979,\n",
            "        2538.9333, 2507.5005, 2541.6570, 2504.9268, 2492.1843, 2516.7617,\n",
            "        2499.8875, 2467.7434, 2533.5457, 2524.6824, 2502.3796, 2551.8398,\n",
            "        2508.1826, 2547.2498, 2512.5317, 2537.7761, 2495.5359, 2503.7341,\n",
            "        2478.9946, 2504.7781, 2533.9634, 2524.0483, 2520.4736, 2525.4404,\n",
            "        2509.2285, 2567.3203, 2491.8943, 2507.2212, 2496.9939, 2507.1743,\n",
            "        2509.4917, 2537.3740, 2540.9255, 2495.3743, 2511.5193, 2496.3228,\n",
            "        2525.6792, 2482.6638, 2541.3545, 2503.2002, 2459.9624, 2541.5974,\n",
            "        2537.7969, 2494.4751, 2530.2891, 2478.0254, 2475.0972, 2526.1047,\n",
            "        2522.1677, 2502.6008, 2521.4973, 2449.2214, 2493.0222, 2506.3208,\n",
            "        2491.9038, 2520.9895, 2509.9551, 2520.7104, 2498.9380, 2541.0068,\n",
            "        2480.5369, 2521.6458, 2498.9905, 2466.4099, 2488.5676, 2515.8020,\n",
            "        2487.0564, 2555.3262, 2500.5923, 2534.5068, 2496.8596, 2555.0125,\n",
            "        2480.9990, 2514.2031, 2533.3850, 2530.2356, 2514.7122, 2499.4348,\n",
            "        2542.8279, 2499.5156, 2501.6377, 2530.1528, 2486.1782, 2474.6318,\n",
            "        2500.1895, 2512.1997, 2519.2019, 2510.9075, 2511.4116, 2542.1821,\n",
            "        2518.5630, 2550.6941, 2570.4460, 2489.6194, 2504.0283, 2466.3630,\n",
            "        2520.4194, 2496.0098, 2462.4436, 2508.6958, 2533.2678, 2486.4292,\n",
            "        2519.3154, 2523.9170, 2553.9304, 2505.2266, 2516.4678, 2495.2678,\n",
            "        2528.6201, 2506.9041, 2528.6370, 2490.7708, 2521.6365, 2508.8511,\n",
            "        2490.7283, 2519.0823, 2511.3872, 2483.5232, 2514.2676, 2521.6477,\n",
            "        2496.5342, 2546.4360, 2537.2036, 2524.0283, 2526.0310, 2569.7637,\n",
            "        2530.8093, 2510.4958, 2509.0151, 2473.9473, 2519.6990, 2484.7683,\n",
            "        2501.4231, 2492.2061, 2468.6431, 2555.4724, 2545.6206, 2534.5264,\n",
            "        2486.2322, 2506.9570, 2491.1487, 2470.5513, 2510.7781, 2501.8210,\n",
            "        2471.3049, 2513.4131, 2487.0264, 2492.6484, 2487.0186, 2525.3313,\n",
            "        2496.2827, 2511.6421, 2508.5481, 2466.8015, 2485.0806, 2510.2717,\n",
            "        2510.8135, 2535.1213, 2501.5803, 2524.6521, 2532.8511, 2521.8979,\n",
            "        2465.6821, 2489.4353, 2551.8679, 2516.6587, 2482.4475, 2529.6360,\n",
            "        2495.1875, 2533.5955, 2512.1777, 2491.7808, 2511.3027, 2523.7222,\n",
            "        2495.4536, 2518.3594, 2506.0364, 2487.6252, 2490.7615, 2499.9111,\n",
            "        2525.2449, 2487.4250, 2524.7112, 2481.2930, 2500.2156, 2545.6521,\n",
            "        2500.6797, 2555.9658, 2554.6724, 2520.5366, 2523.2002, 2533.9624,\n",
            "        2509.4160, 2522.4089, 2500.9487, 2509.1003, 2536.7695, 2532.0588,\n",
            "        2501.1843, 2523.3926, 2505.5017, 2517.2361, 2508.6377, 2500.1997,\n",
            "        2541.0369, 2505.9172, 2515.0454, 2520.7087, 2541.2302, 2482.9680,\n",
            "        2464.8838, 2507.4141, 2491.1562, 2523.0759, 2523.6553, 2520.1455,\n",
            "        2499.9126, 2502.1287, 2511.2556, 2493.3975, 2518.1257, 2504.4956,\n",
            "        2547.3867, 2507.8933, 2536.2117, 2485.2493, 2503.6160, 2509.7815,\n",
            "        2506.7781, 2493.8735, 2473.2412, 2551.0244, 2501.3032, 2481.6760,\n",
            "        2471.7930, 2512.2590, 2525.6719, 2520.5698, 2500.3652, 2481.8657,\n",
            "        2519.2183, 2510.3977, 2545.4702, 2515.8416, 2532.5784, 2508.4922,\n",
            "        2546.4668, 2530.9075, 2509.1985, 2507.5652, 2511.8669, 2447.5691,\n",
            "        2499.0693, 2538.2720, 2504.4702, 2525.2402, 2522.0542, 2522.2085,\n",
            "        2511.6987, 2493.6592, 2539.7703, 2496.3677, 2503.5691, 2498.5093,\n",
            "        2530.6245, 2538.6719, 2505.6191, 2550.1121, 2510.4116, 2498.8142,\n",
            "        2509.1284, 2509.3296, 2511.3489, 2516.9866, 2490.1091, 2487.3521,\n",
            "        2529.8958, 2504.7822, 2511.9634, 2534.9026, 2530.4089, 2500.4741,\n",
            "        2521.5063, 2520.4529, 2536.5066, 2524.6941, 2505.1531, 2520.1118,\n",
            "        2552.1204, 2532.3896, 2533.6018, 2513.1196, 2497.5435, 2506.3125,\n",
            "        2546.6521, 2502.0063, 2504.6174, 2504.0139, 2515.1479, 2470.4421,\n",
            "        2526.5286, 2513.4885, 2490.1555, 2514.0569, 2496.6204, 2496.5938,\n",
            "        2544.6050, 2539.9182, 2507.3984, 2543.1646, 2468.3367, 2507.2815,\n",
            "        2478.9854, 2465.4841, 2516.8435, 2495.7166, 2469.8555, 2522.6912,\n",
            "        2490.7590, 2524.0562, 2486.4788, 2509.3535, 2465.2500, 2500.8047,\n",
            "        2515.2156, 2513.3105, 2484.6978, 2505.6265, 2511.7432, 2523.9153,\n",
            "        2531.8120, 2493.9646, 2529.7480, 2515.9600, 2497.5300, 2520.2046,\n",
            "        2511.1094, 2500.7830, 2461.4780, 2493.6782, 2496.8184, 2529.8928,\n",
            "        2493.5564, 2487.8054, 2530.1252, 2509.6167, 2512.4141, 2541.4934,\n",
            "        2553.4465, 2501.2646, 2553.7402, 2486.0183, 2522.2629, 2488.0427,\n",
            "        2499.1458, 2473.5891, 2506.4370, 2522.7769, 2527.3230, 2496.3118,\n",
            "        2453.8435, 2508.3108, 2493.4058, 2518.7275, 2489.4158, 2511.8374,\n",
            "        2521.7788, 2509.9399, 2520.8818, 2487.8372, 2509.5457, 2505.6216,\n",
            "        2518.0503, 2508.8755, 2516.0977, 2480.5210, 2531.3718, 2510.8018,\n",
            "        2515.0845, 2498.8809, 2502.5684, 2507.2400, 2493.6956, 2540.2917,\n",
            "        2515.1570, 2502.6528, 2523.5464, 2478.4248, 2487.2432, 2538.9556,\n",
            "        2495.4277, 2499.4758, 2489.8340, 2482.2322, 2538.6206, 2514.1858,\n",
            "        2504.4033, 2527.4138, 2518.7212, 2554.1704, 2545.4863, 2525.1729,\n",
            "        2495.9932, 2500.7351, 2545.7202, 2497.1543, 2488.0361, 2528.9902,\n",
            "        2541.9465, 2502.4722, 2512.9248, 2540.8599, 2496.5217, 2511.3762,\n",
            "        2496.5938, 2492.4180, 2509.8750, 2514.4753, 2496.3506, 2553.2373,\n",
            "        2522.0581, 2495.6802, 2511.0674, 2493.8936, 2553.1780, 2491.0632,\n",
            "        2517.0391, 2497.3760, 2490.8757, 2515.6003, 2499.1790, 2493.9224,\n",
            "        2507.4001, 2531.1101, 2486.7827, 2501.9133, 2482.8174, 2545.8721,\n",
            "        2504.8784, 2500.3152, 2480.8521, 2497.6396, 2538.1377, 2539.0715,\n",
            "        2496.0100, 2502.0754, 2490.7905, 2494.3240, 2514.1165, 2545.5886,\n",
            "        2499.4602, 2525.5696, 2503.6052, 2524.9238, 2506.8027, 2490.0486,\n",
            "        2542.0886, 2498.6033, 2528.3225, 2496.4272, 2489.3235, 2520.8743,\n",
            "        2483.2239, 2575.2715, 2546.7371, 2515.6372, 2506.7417, 2474.6497,\n",
            "        2494.9390, 2489.5500, 2523.7915, 2502.6360, 2534.3296, 2501.3435,\n",
            "        2524.7595, 2497.9785, 2470.0081, 2488.5723, 2496.6597, 2517.8167,\n",
            "        2508.7527, 2442.3872, 2532.4548, 2491.8582, 2495.6282, 2503.2246,\n",
            "        2532.4866, 2491.6670, 2497.0608, 2507.9414, 2509.3877, 2504.3748,\n",
            "        2517.7104, 2535.8643, 2535.5850, 2524.1431, 2486.1265, 2516.1382,\n",
            "        2522.3411, 2490.5183, 2502.6624, 2495.0303, 2562.4783, 2505.9702,\n",
            "        2489.6868, 2509.3833, 2530.8618, 2505.3862, 2481.8398, 2489.6323,\n",
            "        2495.1431, 2500.9792, 2541.7712, 2487.6694, 2476.2808, 2492.1033,\n",
            "        2489.7852, 2471.5806, 2517.3000, 2507.1309, 2526.5813, 2498.2097,\n",
            "        2491.9939, 2456.0564, 2533.3760, 2528.6860, 2550.6655, 2557.6841,\n",
            "        2503.1704, 2513.5356, 2523.7822, 2491.0332, 2515.8384, 2511.9790,\n",
            "        2488.8174, 2504.0352, 2508.0027, 2540.3103, 2522.2068, 2489.2029,\n",
            "        2536.8489, 2540.0815, 2496.3994, 2507.8311, 2552.2146, 2505.8337,\n",
            "        2511.3840, 2502.8066, 2503.4727, 2507.6338, 2500.0981, 2483.5312,\n",
            "        2523.7693, 2537.2981, 2523.3450, 2513.4653, 2527.1877, 2514.0198,\n",
            "        2501.6472, 2471.8284, 2483.6748, 2522.4429, 2457.4917, 2510.7087,\n",
            "        2503.8301, 2493.2053, 2500.6533, 2510.6086, 2484.5720, 2530.9949,\n",
            "        2550.8865, 2490.5688, 2536.7974, 2515.5901, 2511.2764, 2538.9233,\n",
            "        2537.7139, 2497.8870, 2519.1306, 2540.1306, 2510.3081, 2503.5173,\n",
            "        2469.5554, 2501.5996, 2520.6235, 2509.5437, 2545.7102, 2488.2358,\n",
            "        2535.2654, 2511.5034, 2505.8623, 2526.7271, 2481.9526, 2505.0854,\n",
            "        2487.0117, 2515.8467, 2500.0518, 2495.6846, 2502.7771, 2497.6250,\n",
            "        2520.0769, 2507.3508, 2501.0029, 2512.2788, 2551.9697, 2509.8723,\n",
            "        2495.8291, 2501.0930, 2502.8552, 2525.1729, 2523.0837, 2491.1736,\n",
            "        2502.6396, 2496.5681, 2514.7070, 2521.3853, 2473.9048, 2466.2854,\n",
            "        2516.8750, 2472.6907, 2505.1187, 2501.8716, 2521.6685, 2526.4641,\n",
            "        2493.8291, 2485.1399, 2556.3203, 2531.1062, 2496.9382, 2505.0537,\n",
            "        2509.6877, 2516.9397, 2500.1318, 2509.0718, 2493.9993, 2529.3916,\n",
            "        2521.8726, 2469.6687, 2505.1384, 2535.4089, 2513.8696, 2524.1763,\n",
            "        2491.2043, 2526.2473, 2521.0776, 2498.2195, 2496.2534, 2476.6472,\n",
            "        2526.0925, 2507.2620, 2472.5674, 2493.5095, 2496.0706, 2477.1357,\n",
            "        2521.0801, 2523.0452, 2500.5925, 2530.6340, 2503.2351, 2516.1624,\n",
            "        2524.6360, 2525.4150, 2499.4001, 2490.2087, 2505.4993, 2476.3101,\n",
            "        2495.4893, 2524.6807, 2526.5298, 2497.1650, 2507.6804, 2498.3208,\n",
            "        2496.2000, 2505.2964, 2529.1865, 2540.2710, 2533.8586, 2498.9495,\n",
            "        2492.4563, 2522.4360, 2510.5857, 2484.2888, 2494.3127, 2502.7676,\n",
            "        2526.2986, 2488.6660, 2501.0823, 2534.7600, 2548.1909, 2495.5071,\n",
            "        2509.2920, 2515.4771, 2513.3015, 2480.1758, 2475.8574, 2511.3901,\n",
            "        2496.6912, 2492.3162, 2523.3511, 2516.3323, 2508.0291, 2501.6743,\n",
            "        2531.2356, 2532.6963, 2537.4395, 2464.6897, 2541.1660, 2514.4163,\n",
            "        2500.0393, 2490.1238, 2514.0332, 2526.5901, 2546.1165, 2501.4033,\n",
            "        2479.7861, 2511.3567, 2496.6921, 2542.1804, 2544.4636, 2500.5674,\n",
            "        2511.0361, 2501.1548, 2492.0085, 2517.7593, 2510.6252, 2502.3027,\n",
            "        2484.3296, 2490.5493, 2484.0757, 2510.4475, 2489.6990, 2497.4556,\n",
            "        2492.9465, 2499.7998, 2497.1855, 2568.0928, 2492.2554, 2523.5918,\n",
            "        2544.0132, 2523.2976, 2513.3555, 2495.7144, 2520.5996, 2507.3032,\n",
            "        2484.5566, 2519.1919, 2485.5969, 2534.8474, 2500.4417, 2492.3755,\n",
            "        2480.7720, 2519.1946, 2536.7859, 2499.0000, 2507.0344, 2546.7205,\n",
            "        2508.9539, 2532.7192, 2488.4744, 2532.8494, 2481.8025, 2501.1938,\n",
            "        2490.7075, 2497.8843, 2502.6619, 2483.9910, 2515.8074, 2495.0334,\n",
            "        2527.9807, 2465.3301, 2542.3850, 2475.4902, 2529.2788, 2508.1851,\n",
            "        2497.6384, 2504.3633, 2505.5637, 2478.8828, 2498.9409, 2476.7996,\n",
            "        2470.9116, 2505.9709, 2521.4590, 2510.2136, 2500.1770, 2511.7424,\n",
            "        2486.1365, 2473.5078, 2482.5349, 2487.0005, 2502.8767, 2530.1936,\n",
            "        2518.4714, 2507.0918, 2507.8325, 2551.3682, 2516.2920, 2542.5623,\n",
            "        2531.0906, 2487.6436, 2513.9849, 2518.2473, 2516.8877, 2548.8494,\n",
            "        2541.0259, 2502.7363, 2511.5591, 2502.5789, 2513.9829, 2534.9868,\n",
            "        2510.5349, 2519.0635, 2498.7917, 2501.1057, 2531.9153, 2502.2117,\n",
            "        2503.1704, 2509.3391, 2476.2244, 2514.2070, 2470.5713, 2501.8513,\n",
            "        2511.7336, 2508.7388, 2514.1680, 2482.3767, 2504.5649, 2528.4128,\n",
            "        2511.5488, 2517.7737, 2492.2351, 2525.7725, 2498.7930, 2513.5342,\n",
            "        2487.8938, 2482.7266, 2531.6313, 2499.2515, 2507.9861, 2528.6467,\n",
            "        2536.7070, 2489.0190, 2488.4800, 2514.8491, 2493.0942, 2495.0200,\n",
            "        2506.3945, 2536.4736, 2478.1726, 2482.1423, 2537.8257, 2528.8955,\n",
            "        2537.0120, 2513.4656, 2488.1196, 2485.1121, 2466.4448, 2530.3975,\n",
            "        2489.4700, 2552.4121, 2524.2712, 2504.8496, 2561.9021, 2487.5537,\n",
            "        2506.5164, 2530.3508, 2492.6689, 2504.8108, 2507.2632, 2550.0652,\n",
            "        2547.5537, 2539.9919, 2494.4873, 2503.5278, 2513.5945, 2499.5642,\n",
            "        2496.8196, 2511.2498, 2479.2383, 2491.5615, 2483.6045, 2519.8267,\n",
            "        2504.5994, 2506.3230, 2505.1106, 2514.5190, 2486.1287, 2518.4502,\n",
            "        2495.4585, 2508.0586, 2488.2842, 2495.8538, 2494.5222, 2461.4685,\n",
            "        2494.6550, 2529.5276, 2499.4263, 2487.7739, 2529.0308, 2494.0066,\n",
            "        2516.2668, 2478.9607, 2511.4226, 2529.4766, 2486.7842, 2474.5437,\n",
            "        2523.0608, 2495.4629, 2518.9016, 2515.9119, 2501.1870, 2509.3010,\n",
            "        2522.7620, 2529.3076, 2500.8167, 2488.3533, 2479.5098, 2496.0176,\n",
            "        2506.1716, 2558.3528, 2514.8154, 2522.7212, 2525.5051, 2521.0037,\n",
            "        2520.7949, 2493.2153, 2494.2771, 2496.7786, 2521.0852, 2510.1499,\n",
            "        2545.6255, 2534.5618, 2514.1521, 2497.2166, 2480.5852, 2525.0154,\n",
            "        2513.3083, 2512.2930, 2543.9470, 2522.9182, 2509.9792, 2491.6355,\n",
            "        2531.0569, 2492.3467, 2499.8452, 2531.8379, 2508.2974, 2489.3369,\n",
            "        2468.3218, 2492.7539, 2527.1001, 2516.1765, 2506.4607, 2518.4744,\n",
            "        2554.2422, 2524.5789, 2534.8684, 2479.3835, 2470.4287, 2536.8076,\n",
            "        2526.9646, 2527.9988, 2519.0654, 2507.8992, 2562.9126, 2479.4885,\n",
            "        2508.1394, 2526.0425, 2518.7808, 2505.2537, 2502.7576, 2503.8745,\n",
            "        2466.1274, 2480.8184, 2469.8335, 2535.4817, 2525.0874, 2529.3130,\n",
            "        2481.1206, 2513.7104, 2464.0134, 2503.5354, 2484.1460, 2523.1677,\n",
            "        2507.2258, 2516.4534, 2485.3811, 2531.8860, 2485.0269, 2512.2786,\n",
            "        2501.3640, 2541.8206, 2490.6960, 2499.3008], device='mps:0')\n",
            "Total time on mps is 0.11147380 seconds\n"
          ]
        }
      ],
      "source": [
        "r = torch.empty(1000,device=device)\n",
        "torch.mps.synchronize()\n",
        "start = time.time()\n",
        "for i in range(1000):\n",
        "    A = torch.rand(100,100,device=device)\n",
        "    B = torch.rand(100,100,device=device)\n",
        "    res = torch.mm(A,B)\n",
        "    r[i] = torch.norm(res,p='fro')\n",
        "torch.mps.synchronize()\n",
        "end = time.time()\n",
        "time4 = end - start\n",
        "print(f'r is {r}')\n",
        "print(f'Total time on {device} is {time4:.8f} seconds')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performance boost : 3.923782228828694x\n",
            "Reduction Percentage : 74.514385822617%\n"
          ]
        }
      ],
      "source": [
        "perfBoost = time3 / time4 \n",
        "print(f'Performance boost : {perfBoost}x')\n",
        "\n",
        "reductionPerc = (time3-time4)/time3 * 100\n",
        "print(f'Reduction Percentage : {reductionPerc}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### iv) Observations\n",
        "There's a performance boast of <b>3.92x</b> with a relative reduction percentage of <b>74.51%</b>. This is because print statements take time to execute and are done on the CPU. In the second situation where only one print statement is required in the end, we observe uninterrupted GPU utilization, while CPU-GPU synchronization was required in the former case, slowing down execution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMEcdAp3-uT-"
      },
      "source": [
        "## <font color = 'blue'> **Question 3. Quadratic Regression** </font>\n",
        "\n",
        "In the lecture we discussed a simple regression problem, where points are coming from the line $y = 2x + 1$, plus some noise. For this question you are asked to:\n",
        "\n",
        "(i) Generate a dataset $D$ coming from a quadratic function: $y = -x^2 + 3x + 10$, plus some Gaussian noise with standard deviation 0.5. Here $x$ should range in $[-10,10]$ with 1000 data points. <br>\n",
        "\n",
        "(ii) Create a dataloader for the set $D$, with a batch size of your choice. <br>\n",
        "\n",
        "(iii) Define a model with a forward function that takes as input a scalar $x$, and returns a value $y = w_2 x^2 + w_1x + w_0$, where $w_i$ are the model parameters. The forward function should create polynomial features $[x, x^2]$ and apply a linear layer to produce the output. <br>\n",
        "\n",
        "(iv) Write a training loop that adjusts the parameters of the model in order to fit the data $D$, using the Mean Squared Error loss and SGD optimizer, with a learning rate of 0.1. The training should take place on the GPU. <br>\n",
        "\n",
        "(v) Train your model and report what parameter values it computes. (Sanity check: These should be close to $w_0 = 10$, $w_1 = 3$, $w_2 = -1$) <br>\n",
        "\n",
        "(vi) Use a learning rate scheduler (e.g., StepLR or ExponentialLR) to decay the learning rate during training. <br>\n",
        "\n",
        "(vii) The training loop should collect the values of the loss for each epoch. Visualize the loss by epoch of training for two experiments: with and without the scheduler. What do you observe? <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.manual_seed(33)\n",
        "\n",
        "xTensor = torch.linspace(-10,10,1000)\n",
        "epsilon = torch.randn(1000)*0.5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "yTensor = -(xTensor)**2 + 3*xTensor + 10 + epsilon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean:  tensor(-3.0518e-08)\n",
            "std:  tensor(5.7822)\n"
          ]
        }
      ],
      "source": [
        "xMean = xTensor.mean()\n",
        "print('mean: ',xMean)\n",
        "xStd = xTensor.std()\n",
        "print('std: ',xStd)\n",
        "normalizedX = (xTensor - xMean) / xStd\n",
        "# did it because my values were exploding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial state without scheduler :  OrderedDict({'linear.weight': tensor([[0.1677, 0.0831]], device='mps:0'), 'linear.bias': tensor([-0.4332], device='mps:0')})\n",
            "Final state without scheduler :  OrderedDict({'linear.weight': tensor([[ 17.3803, -33.4197]], device='mps:0'), 'linear.bias': tensor([9.9856], device='mps:0')})\n",
            "Initial state with StepLR scheduler :  OrderedDict({'linear.weight': tensor([[-0.5184, -0.0966]], device='mps:0'), 'linear.bias': tensor([-0.0819], device='mps:0')})\n",
            "Final state with StepLR scheduler :  OrderedDict({'linear.weight': tensor([[ 17.3437, -32.0753]], device='mps:0'), 'linear.bias': tensor([7.9527], device='mps:0')})\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(33)\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, tensorX, tensorY):\n",
        "        self.x = tensorX\n",
        "        self.y = tensorY\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return (self.x[index],self.y[index])\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "    \n",
        "class QuadraticRegression(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(2,1)\n",
        "        \n",
        "    def forward(self,x):\n",
        "        x = x.view(-1,1) \n",
        "        x2 = x**2\n",
        "        features = torch.cat([x,x2],dim=1)\n",
        "        return self.linear(features)    \n",
        "    \n",
        "def makeTrainStep(model,lossFunction,optimizer):\n",
        "    def trainStep(x,y):\n",
        "        model.train()\n",
        "        pred = model(x)\n",
        "        #print(pred.shape)\n",
        "        y = y.view(-1,1)\n",
        "        #print(y.shape)\n",
        "        loss = lossFunction(pred,y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        return loss.item()\n",
        "    return trainStep\n",
        "\n",
        "def trainEpochs(epochs,trainLoader,trainStep,scheduler=None):\n",
        "    losses = []\n",
        "    for _ in range(epochs):\n",
        "        epochLoss = 0.0\n",
        "        for xBatch, yBatch in trainLoader:\n",
        "            xBatch = xBatch.to(device)\n",
        "            yBatch = yBatch.to(device)\n",
        "            loss = trainStep(xBatch,yBatch)\n",
        "            epochLoss += loss\n",
        "        losses.append(epochLoss)\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "    return losses\n",
        "\n",
        "\n",
        "trainData = CustomDataset(normalizedX,yTensor)\n",
        "trainLoader = DataLoader(trainData,batch_size=32,shuffle=True)\n",
        "\n",
        "lossFunction = nn.MSELoss()\n",
        "\n",
        "model = QuadraticRegression().to(device)\n",
        "optimizer = optim.SGD(model.parameters(),lr=0.1)\n",
        "print('Initial state without scheduler : ',model.state_dict())\n",
        "trainStep = makeTrainStep(model,lossFunction,optimizer)\n",
        "lossesWithoutScheduler = trainEpochs(10,trainLoader,trainStep)\n",
        "print('Final state without scheduler : ',model.state_dict())\n",
        "\n",
        "model2 = QuadraticRegression().to(device)\n",
        "optimizer2 = optim.SGD(model2.parameters(),lr=0.1)\n",
        "stepLR = optim.lr_scheduler.StepLR(optimizer2,step_size=1,gamma=0.1)\n",
        "print('Initial state with StepLR scheduler : ',model2.state_dict())\n",
        "trainStep = makeTrainStep(model2,lossFunction,optimizer2)\n",
        "lossesWithStepLR = trainEpochs(10,trainLoader,trainStep,stepLR)\n",
        "print('Final state with StepLR scheduler : ',model2.state_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, its important to note that the model learns an equation for xNormalized and y, not x and y. The equation learned by the model can be expressed as : <br>\n",
        "\n",
        "$ y = xNorm * w1 + (xNorm^2) * w2 + bias$ where $xNorm = (x - \\mu) / \\sigma$\n",
        "\n",
        "$ \\implies y = ((x - \\mu) / \\sigma)* w1 + (((x - \\mu) / \\sigma)^2) * w2 + bias $\n",
        "\n",
        "$ \\implies y = x^2 * (w2 / \\sigma^2) + x * (w1 / \\sigma - 2*\\mu*w2 / \\sigma^2) + ( \\mu^2*w2 / \\sigma^2 - \\mu * w1 / \\sigma + bias ) $\n",
        "\n",
        "where, <br>\n",
        "weights [w1,w2]: tensor([[ 17.3803, -33.4197]], device='mps:0') <br>\n",
        "bias: tensor([9.9856], device='mps:0') <br>\n",
        "mean ($\\mu$):  tensor(-3.0518e-08) <br>\n",
        "std ($\\sigma$):  tensor(5.7822) <br>\n",
        "\n",
        "Given that the mean is so small, we approximate it to 0. \n",
        "\n",
        "$ \\implies y = x^2 * (-33.4197 / (5.7822) ^ 2) + x * (17.3803 / 5.7822) + 9.9856 $\n",
        "\n",
        "$ \\implies y = x^2 * (-0.99) + x * 3.005 + 9.9856 $"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.plotly.v1+json": {
              "config": {
                "linkText": "Export to plot.ly",
                "plotlyServerURL": "https://plot.ly",
                "showLink": false
              },
              "data": [
                {
                  "mode": "lines",
                  "name": "Without Scheduler",
                  "type": "scatter",
                  "x": [
                    0,
                    1,
                    2,
                    3,
                    4,
                    5,
                    6,
                    7,
                    8,
                    9
                  ],
                  "y": [
                    4409.742153167725,
                    35.397270023822784,
                    8.519376665353775,
                    8.186953648924828,
                    8.188173994421959,
                    8.159452468156815,
                    8.01398603618145,
                    8.28559635579586,
                    8.152383178472519,
                    8.181664496660233
                  ]
                },
                {
                  "mode": "lines",
                  "name": "With StepLR Scheduler",
                  "type": "scatter",
                  "x": [
                    0,
                    1,
                    2,
                    3,
                    4,
                    5,
                    6,
                    7,
                    8,
                    9
                  ],
                  "y": [
                    4478.535426616669,
                    87.65775299072266,
                    69.97781944274902,
                    68.67755734920502,
                    68.183469414711,
                    67.88875806331635,
                    68.70351707935333,
                    68.32181286811829,
                    67.92993152141571,
                    69.28297781944275
                  ]
                }
              ],
              "layout": {
                "template": {
                  "data": {
                    "bar": [
                      {
                        "error_x": {
                          "color": "#2a3f5f"
                        },
                        "error_y": {
                          "color": "#2a3f5f"
                        },
                        "marker": {
                          "line": {
                            "color": "#E5ECF6",
                            "width": 0.5
                          },
                          "pattern": {
                            "fillmode": "overlay",
                            "size": 10,
                            "solidity": 0.2
                          }
                        },
                        "type": "bar"
                      }
                    ],
                    "barpolar": [
                      {
                        "marker": {
                          "line": {
                            "color": "#E5ECF6",
                            "width": 0.5
                          },
                          "pattern": {
                            "fillmode": "overlay",
                            "size": 10,
                            "solidity": 0.2
                          }
                        },
                        "type": "barpolar"
                      }
                    ],
                    "carpet": [
                      {
                        "aaxis": {
                          "endlinecolor": "#2a3f5f",
                          "gridcolor": "white",
                          "linecolor": "white",
                          "minorgridcolor": "white",
                          "startlinecolor": "#2a3f5f"
                        },
                        "baxis": {
                          "endlinecolor": "#2a3f5f",
                          "gridcolor": "white",
                          "linecolor": "white",
                          "minorgridcolor": "white",
                          "startlinecolor": "#2a3f5f"
                        },
                        "type": "carpet"
                      }
                    ],
                    "choropleth": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "type": "choropleth"
                      }
                    ],
                    "contour": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ],
                        "type": "contour"
                      }
                    ],
                    "contourcarpet": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "type": "contourcarpet"
                      }
                    ],
                    "heatmap": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ],
                        "type": "heatmap"
                      }
                    ],
                    "histogram": [
                      {
                        "marker": {
                          "pattern": {
                            "fillmode": "overlay",
                            "size": 10,
                            "solidity": 0.2
                          }
                        },
                        "type": "histogram"
                      }
                    ],
                    "histogram2d": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ],
                        "type": "histogram2d"
                      }
                    ],
                    "histogram2dcontour": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ],
                        "type": "histogram2dcontour"
                      }
                    ],
                    "mesh3d": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "type": "mesh3d"
                      }
                    ],
                    "parcoords": [
                      {
                        "line": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "parcoords"
                      }
                    ],
                    "pie": [
                      {
                        "automargin": true,
                        "type": "pie"
                      }
                    ],
                    "scatter": [
                      {
                        "fillpattern": {
                          "fillmode": "overlay",
                          "size": 10,
                          "solidity": 0.2
                        },
                        "type": "scatter"
                      }
                    ],
                    "scatter3d": [
                      {
                        "line": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scatter3d"
                      }
                    ],
                    "scattercarpet": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scattercarpet"
                      }
                    ],
                    "scattergeo": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scattergeo"
                      }
                    ],
                    "scattergl": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scattergl"
                      }
                    ],
                    "scattermap": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scattermap"
                      }
                    ],
                    "scattermapbox": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scattermapbox"
                      }
                    ],
                    "scatterpolar": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scatterpolar"
                      }
                    ],
                    "scatterpolargl": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scatterpolargl"
                      }
                    ],
                    "scatterternary": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scatterternary"
                      }
                    ],
                    "surface": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ],
                        "type": "surface"
                      }
                    ],
                    "table": [
                      {
                        "cells": {
                          "fill": {
                            "color": "#EBF0F8"
                          },
                          "line": {
                            "color": "white"
                          }
                        },
                        "header": {
                          "fill": {
                            "color": "#C8D4E3"
                          },
                          "line": {
                            "color": "white"
                          }
                        },
                        "type": "table"
                      }
                    ]
                  },
                  "layout": {
                    "annotationdefaults": {
                      "arrowcolor": "#2a3f5f",
                      "arrowhead": 0,
                      "arrowwidth": 1
                    },
                    "autotypenumbers": "strict",
                    "coloraxis": {
                      "colorbar": {
                        "outlinewidth": 0,
                        "ticks": ""
                      }
                    },
                    "colorscale": {
                      "diverging": [
                        [
                          0,
                          "#8e0152"
                        ],
                        [
                          0.1,
                          "#c51b7d"
                        ],
                        [
                          0.2,
                          "#de77ae"
                        ],
                        [
                          0.3,
                          "#f1b6da"
                        ],
                        [
                          0.4,
                          "#fde0ef"
                        ],
                        [
                          0.5,
                          "#f7f7f7"
                        ],
                        [
                          0.6,
                          "#e6f5d0"
                        ],
                        [
                          0.7,
                          "#b8e186"
                        ],
                        [
                          0.8,
                          "#7fbc41"
                        ],
                        [
                          0.9,
                          "#4d9221"
                        ],
                        [
                          1,
                          "#276419"
                        ]
                      ],
                      "sequential": [
                        [
                          0,
                          "#0d0887"
                        ],
                        [
                          0.1111111111111111,
                          "#46039f"
                        ],
                        [
                          0.2222222222222222,
                          "#7201a8"
                        ],
                        [
                          0.3333333333333333,
                          "#9c179e"
                        ],
                        [
                          0.4444444444444444,
                          "#bd3786"
                        ],
                        [
                          0.5555555555555556,
                          "#d8576b"
                        ],
                        [
                          0.6666666666666666,
                          "#ed7953"
                        ],
                        [
                          0.7777777777777778,
                          "#fb9f3a"
                        ],
                        [
                          0.8888888888888888,
                          "#fdca26"
                        ],
                        [
                          1,
                          "#f0f921"
                        ]
                      ],
                      "sequentialminus": [
                        [
                          0,
                          "#0d0887"
                        ],
                        [
                          0.1111111111111111,
                          "#46039f"
                        ],
                        [
                          0.2222222222222222,
                          "#7201a8"
                        ],
                        [
                          0.3333333333333333,
                          "#9c179e"
                        ],
                        [
                          0.4444444444444444,
                          "#bd3786"
                        ],
                        [
                          0.5555555555555556,
                          "#d8576b"
                        ],
                        [
                          0.6666666666666666,
                          "#ed7953"
                        ],
                        [
                          0.7777777777777778,
                          "#fb9f3a"
                        ],
                        [
                          0.8888888888888888,
                          "#fdca26"
                        ],
                        [
                          1,
                          "#f0f921"
                        ]
                      ]
                    },
                    "colorway": [
                      "#636efa",
                      "#EF553B",
                      "#00cc96",
                      "#ab63fa",
                      "#FFA15A",
                      "#19d3f3",
                      "#FF6692",
                      "#B6E880",
                      "#FF97FF",
                      "#FECB52"
                    ],
                    "font": {
                      "color": "#2a3f5f"
                    },
                    "geo": {
                      "bgcolor": "white",
                      "lakecolor": "white",
                      "landcolor": "#E5ECF6",
                      "showlakes": true,
                      "showland": true,
                      "subunitcolor": "white"
                    },
                    "hoverlabel": {
                      "align": "left"
                    },
                    "hovermode": "closest",
                    "mapbox": {
                      "style": "light"
                    },
                    "paper_bgcolor": "white",
                    "plot_bgcolor": "#E5ECF6",
                    "polar": {
                      "angularaxis": {
                        "gridcolor": "white",
                        "linecolor": "white",
                        "ticks": ""
                      },
                      "bgcolor": "#E5ECF6",
                      "radialaxis": {
                        "gridcolor": "white",
                        "linecolor": "white",
                        "ticks": ""
                      }
                    },
                    "scene": {
                      "xaxis": {
                        "backgroundcolor": "#E5ECF6",
                        "gridcolor": "white",
                        "gridwidth": 2,
                        "linecolor": "white",
                        "showbackground": true,
                        "ticks": "",
                        "zerolinecolor": "white"
                      },
                      "yaxis": {
                        "backgroundcolor": "#E5ECF6",
                        "gridcolor": "white",
                        "gridwidth": 2,
                        "linecolor": "white",
                        "showbackground": true,
                        "ticks": "",
                        "zerolinecolor": "white"
                      },
                      "zaxis": {
                        "backgroundcolor": "#E5ECF6",
                        "gridcolor": "white",
                        "gridwidth": 2,
                        "linecolor": "white",
                        "showbackground": true,
                        "ticks": "",
                        "zerolinecolor": "white"
                      }
                    },
                    "shapedefaults": {
                      "line": {
                        "color": "#2a3f5f"
                      }
                    },
                    "ternary": {
                      "aaxis": {
                        "gridcolor": "white",
                        "linecolor": "white",
                        "ticks": ""
                      },
                      "baxis": {
                        "gridcolor": "white",
                        "linecolor": "white",
                        "ticks": ""
                      },
                      "bgcolor": "#E5ECF6",
                      "caxis": {
                        "gridcolor": "white",
                        "linecolor": "white",
                        "ticks": ""
                      }
                    },
                    "title": {
                      "x": 0.05
                    },
                    "xaxis": {
                      "automargin": true,
                      "gridcolor": "white",
                      "linecolor": "white",
                      "ticks": "",
                      "title": {
                        "standoff": 15
                      },
                      "zerolinecolor": "white",
                      "zerolinewidth": 2
                    },
                    "yaxis": {
                      "automargin": true,
                      "gridcolor": "white",
                      "linecolor": "white",
                      "ticks": "",
                      "title": {
                        "standoff": 15
                      },
                      "zerolinecolor": "white",
                      "zerolinewidth": 2
                    }
                  }
                },
                "title": {
                  "text": "Line Graph"
                },
                "xaxis": {
                  "title": {
                    "text": "Epochs"
                  }
                },
                "yaxis": {
                  "title": {
                    "text": "Loss"
                  }
                }
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import plotly.graph_objs as go\n",
        "import plotly.offline as pyo\n",
        "\n",
        "x = list(range(len(lossesWithoutScheduler)))  \n",
        "trace1 = go.Scatter(x=x, y=lossesWithoutScheduler, mode='lines', name='Without Scheduler')\n",
        "trace2 = go.Scatter(x=x, y=lossesWithStepLR, mode='lines', name='With StepLR Scheduler')\n",
        "\n",
        "layout = go.Layout(title='Line Graph', xaxis=dict(title='Epochs'), yaxis=dict(title='Loss'))\n",
        "fig = go.Figure(data=[trace1, trace2], layout=layout)\n",
        "\n",
        "pyo.iplot(fig) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Observation\n",
        "The loss observed without using a scheduler is lower than the one with it possibly overshooting or converging harder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBjMZF1wGaUp"
      },
      "outputs": [],
      "source": [
        "# for grader use only\n",
        "\n",
        "# insert grade here (from 0 to 28)\n",
        "\n",
        "# G[2] =\n",
        "#\n",
        "# please justify point subtractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZHVWBIjIuoy"
      },
      "outputs": [],
      "source": [
        "# total score\n",
        "max_score = 36\n",
        "final_score = sum(G)*(100/max_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5CaHjIuI67F"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
